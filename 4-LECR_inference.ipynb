{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 4: Inference\n\nUse our finetuned retriever and reranker for inference, then submit to the competition.","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.checkpoint import checkpoint\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\nimport cupy as cp\nfrom cuml.metrics import pairwise_distances\nfrom cuml.neighbors import NearestNeighbors\n%env TOKENIZERS_PARALLELISM=false\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# =========================================================================================\n# Configurations\n# =========================================================================================\nclass CFG:\n    print_freq = 3000\n    num_workers = 4\n    uns_model = \"/kaggle/input/lecr-retriever/paraphrase-multilingual-mpnet-base-v2-exp19_fold0_epochs10\"\n    sup_model = \"/kaggle/input/lecr-retriever/paraphrase-multilingual-mpnet-base-v2-exp19_fold0_epochs10\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model)\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model)\n    gradient_checkpointing = False\n    batch_size = 64\n    n_folds = 5\n    top_n = 100\n    seed = 1006\n    threshold = 0.08\n    \n# =========================================================================================\n# Data Loading\n# =========================================================================================\ndef read_data(cfg):\n    topics = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/topics.csv')\n    content = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/content.csv')\n    sample_submission = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv')\n    # Merge topics with sample submission to only infer test topics\n    topics = topics.merge(sample_submission, how = 'inner', left_on = 'id', right_on = 'topic_id')\n    # Fillna titles\n    topics['title'].fillna(\"\", inplace = True)\n    content['title'].fillna(\"\", inplace = True)\n    # Sort by title length to make inference faster\n    topics['length'] = topics['title'].apply(lambda x: len(x))\n    content['length'] = content['title'].apply(lambda x: len(x))\n    topics.sort_values('length', inplace = True)\n    content.sort_values('length', inplace = True)\n    # Drop cols\n    topics.drop(['description', 'channel', 'category', 'level', 'language', 'parent', 'has_content', 'length', 'topic_id', 'content_ids'], axis = 1, inplace = True)\n    content.drop(['description', 'kind', 'language', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n    # Reset index\n    topics.reset_index(drop = True, inplace = True)\n    content.reset_index(drop = True, inplace = True)\n    return topics, content\n\n# =========================================================================================\n# Prepare input, tokenize\n# =========================================================================================\ndef prepare_uns_input(text, cfg):\n    inputs = cfg.uns_tokenizer.encode_plus(\n        text, \n        return_tensors = None, \n        add_special_tokens = True, \n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n# =========================================================================================\n# Unsupervised dataset\n# =========================================================================================\nclass uns_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['title'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_uns_input(self.texts[item], self.cfg)\n        return inputs\n    \n# =========================================================================================\n# Prepare input, tokenize\n# =========================================================================================\ndef prepare_sup_input(text, cfg):\n    inputs = cfg.sup_tokenizer.encode_plus(\n        text, \n        return_tensors = None, \n        add_special_tokens = True, \n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n# =========================================================================================\n# Supervised dataset\n# =========================================================================================\nclass sup_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['text'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_sup_input(self.texts[item], self.cfg)\n        return inputs\n\n# =========================================================================================\n# Mean pooling class\n# =========================================================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n# =========================================================================================\n# Unsupervised model\n# =========================================================================================\nclass uns_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.uns_model)\n        self.model = AutoModel.from_pretrained(cfg.uns_model, config = self.config)\n        self.pool = MeanPooling()\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        return feature\n    \n# =========================================================================================\n# Get embeddings\n# =========================================================================================\ndef get_embeddings(loader, model, device):\n    model.eval()\n    preds = []\n    for step, inputs in enumerate(tqdm(loader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    preds = np.concatenate(preds)\n    return preds\n\n# =========================================================================================\n# Get the amount of positive classes based on the total\n# =========================================================================================\ndef get_pos_socre(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n    return round(np.mean(int_true), 5)\n\n# =========================================================================================\n# Build our inference set\n# =========================================================================================\ndef build_inference_set(topics, content, cfg):\n    # Create lists for training\n    topics_ids = []\n    content_ids = []\n    title1 = []\n    title2 = []\n    # Iterate over each topic\n    for k in tqdm(range(len(topics))):\n        row = topics.iloc[k]\n        topics_id = row['id']\n        topics_title = row['title']\n        predictions = row['predictions'].split(' ')\n        for pred in predictions:\n            content_title = content.loc[pred, 'title']\n            topics_ids.append(topics_id)\n            content_ids.append(pred)\n            title1.append(topics_title)\n            title2.append(content_title)\n    # Build training dataset\n    test = pd.DataFrame(\n        {'topics_ids': topics_ids, \n         'content_ids': content_ids, \n         'title1': title1, \n         'title2': title2\n        }\n    )\n    # Release memory\n    del topics_ids, content_ids, title1, title2\n    gc.collect()\n    return test\n    \n# =========================================================================================\n# Get neighbors\n# =========================================================================================\ndef get_neighbors(topics, content, cfg):\n    # Create topics dataset\n    topics_dataset = uns_dataset(topics, cfg)\n    # Create content dataset\n    content_dataset = uns_dataset(content, cfg)\n    # Create topics and content dataloaders\n    topics_loader = DataLoader(\n        topics_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n    )\n    content_loader = DataLoader(\n        content_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n        )\n    # Create unsupervised model to extract embeddings\n    model = uns_model(cfg)\n    model.to(device)\n    # Predict topics\n    topics_preds = get_embeddings(topics_loader, model, device)\n    content_preds = get_embeddings(content_loader, model, device)\n    # Transfer predictions to gpu\n    topics_preds_gpu = cp.array(topics_preds)\n    content_preds_gpu = cp.array(content_preds)\n    # Release memory\n    torch.cuda.empty_cache()\n    del topics_dataset, content_dataset, topics_loader, content_loader, topics_preds, content_preds\n    gc.collect()\n    # KNN model\n    print('Training KNN model...')\n    neighbors_model = NearestNeighbors(n_neighbors = cfg.top_n, metric = 'cosine')\n    neighbors_model.fit(content_preds_gpu)\n    indices = neighbors_model.kneighbors(topics_preds_gpu, return_distance = False)\n    predictions = []\n    for k in range(len(indices)):\n        pred = indices[k]\n        p = ' '.join([content.loc[ind, 'id'] for ind in pred])\n        predictions.append(p)\n    topics['predictions'] = predictions\n    # Release memory\n    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n    gc.collect()\n    return topics, content \n\n# =========================================================================================\n# Process test\n# =========================================================================================\ndef preprocess_test(test):\n    test['title1'].fillna(\"Title does not exist\", inplace = True)\n    test['title2'].fillna(\"Title does not exist\", inplace = True)\n    # Create feature column\n    test['text'] = test['title1'] + '[SEP]' + test['title2']\n    # Drop titles\n    test.drop(['title1', 'title2'], axis = 1, inplace = True)\n    # Sort so inference is faster\n    test['length'] = test['text'].apply(lambda x: len(x))\n    test.sort_values('length', inplace = True)\n    test.drop(['length'], axis = 1, inplace = True)\n    test.reset_index(drop = True, inplace = True)\n    gc.collect()\n    return test\n\n# =========================================================================================\n# Model\n# =========================================================================================\nclass custom_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.sup_model, output_hidden_states = True)\n        self.config.hidden_dropout = 0.0\n        self.config.hidden_dropout_prob = 0.0\n        self.config.attention_dropout = 0.0\n        self.config.attention_probs_dropout_prob = 0.0\n        self.model = AutoModel.from_pretrained(cfg.sup_model, config = self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n    \n# =========================================================================================\n# Inference function loop\n# =========================================================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total = len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n    predictions = np.concatenate(preds)\n    return predictions\n\n# =========================================================================================\n\n        \n# Read data\ntopics, content = read_data(CFG)\n# Run nearest neighbors\ntopics, content = get_neighbors(topics, content, CFG)\ngc.collect()\n# Set id as index for content\ncontent.set_index('id', inplace = True)\n# Build training set\ntest = build_inference_set(topics, content, CFG)\n# Process test set\ntest = preprocess_test(test)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-11T14:23:30.031897Z","iopub.execute_input":"2023-02-11T14:23:30.032774Z","iopub.status.idle":"2023-02-11T14:26:52.122933Z","shell.execute_reply.started":"2023-02-11T14:23:30.032676Z","shell.execute_reply":"2023-02-11T14:26:52.121278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference\n# =========================================================================================\ndef inference(test, cfg):\n    # Create dataset and loader\n    test_dataset = sup_dataset(test, cfg)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.sup_tokenizer, padding = 'longest'),\n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n    )\n    # Get model\n    model = custom_model(cfg)\n    # Load weights\n    state = torch.load(\"/kaggle/input/lecr-reranker/autodl-tmp-paraphrase-multilingual-mpnet-base-v2-exp19_fold0_epochs10_fold0_rerank_exp19.pth\", map_location = torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    # Release memory\n    torch.cuda.empty_cache()\n    del test_dataset, test_loader, model, state\n    gc.collect()\n    # Use threshold\n    test['prediction'] = prediction\n    test_1 = test.sort_values('prediction', ascending = False).drop_duplicates('topics_ids', keep = 'first')\n\n    test['predictions'] = np.where(prediction > CFG.threshold, 1, 0)\n    test1 = test[test['predictions'] == 1]\n    test1 = pd.concat([test1, test_1])\n\n    test1 = test1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n    test1['content_ids'] = test1['content_ids'].apply(lambda x: ' '.join(x))\n    test1.columns = ['topic_id', 'content_ids']\n    test0 = pd.Series(test['topics_ids'].unique())\n    test0 = test0[~test0.isin(test1['topic_id'])]\n    test0 = pd.DataFrame({'topic_id': test0.values, 'content_ids': \"\"})\n    test_r = pd.concat([test1, test0], axis = 0, ignore_index = True)\n    test_r.to_csv('submission.csv', index = False)\n    return test_r\n\n# Inference\ntest_r = inference(test, CFG)\ntest_r.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:26:52.126256Z","iopub.execute_input":"2023-02-11T14:26:52.127075Z","iopub.status.idle":"2023-02-11T14:27:29.104721Z","shell.execute_reply.started":"2023-02-11T14:26:52.127022Z","shell.execute_reply":"2023-02-11T14:27:29.102925Z"},"trusted":true},"execution_count":null,"outputs":[]}]}