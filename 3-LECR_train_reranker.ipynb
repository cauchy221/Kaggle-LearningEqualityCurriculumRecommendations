{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109163fc-38f4-4dcc-9711-7ff9e4b02448",
   "metadata": {},
   "source": [
    "# Step 3: Get our reranker\n",
    "\n",
    "Use recalled data from step 2 to continue finetuning our finetuned retriever model in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae9c231-cb2b-4d9a-9251-e5435068b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3774baca-48da-44b1-9010-7e0af6b76404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0dba531-b826-47a2-89bf-6d8165b0929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "    model = 'autodl-tmp/paraphrase-multilingual-mpnet-base-v2-exp19_fold0_epochs10'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    epochs = 5\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 128\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    max_len = 512\n",
    "    n_folds = 5\n",
    "    seed = 1006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d622943c-25d3-4441-889d-8e5449883f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc22cdc-a4b7-4d11-9874-bc9dc5176a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c03ea1-a50e-455f-be51-8ee2f01dfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cfg):\n",
    "    train = pd.read_csv('recall_data_top50_fold0_exp19.csv')\n",
    "    train['title1'].fillna(\"Title does not exist\", inplace = True)\n",
    "    train['title2'].fillna(\"Title does not exist\", inplace = True)\n",
    "    correlations = pd.read_csv('correlations.csv')\n",
    "    # Create feature column\n",
    "    train['text'] = train['title1'] + '[SEP]' + train['title2']\n",
    "    return train, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bd92b5-88a3-4b88-9f0e-c0142755cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(train, cfg):\n",
    "    lengths = []\n",
    "    for text in tqdm(train['text'].fillna(\"\").values, total = len(train)):\n",
    "        length = len(cfg.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    cfg.max_len = max(lengths) + 2 # cls & sep\n",
    "    print(f\"max_len: {cfg.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb1176f-86d5-47f6-9fcc-063ffb86cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text, cfg):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f63fd00-6c2f-469a-adb7-02eff245261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.texts[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390aee51-d10b-448d-9ccc-74a344897e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5c84b0-55b3-416f-b0cf-8dd47a508ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b6ac19-f441-4bcd-b2b8-f0fadae625f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abe6b7c-c675-4531-a697-21529383c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cacfbe93-f3b8-4f53-997e-67869d0512bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea7daf1-94da-4a65-a7b0-a46841b4d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.01, 1, 0.01):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acee84b1-e163-49cf-aca5-3b6d62f7ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_one_fold(train, correlations, fold, cfg):\n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "    # Split train & validation\n",
    "    x_train = train[train['fold'] != fold]\n",
    "    x_val = train[train['fold'] == fold]\n",
    "    valid_labels = x_val['target'].values\n",
    "    train_dataset = custom_dataset(x_train, cfg)\n",
    "    valid_dataset = custom_dataset(x_val, cfg)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = True, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    model.to(device)\n",
    "    # Optimizer\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model, \n",
    "        encoder_lr = cfg.encoder_lr, \n",
    "        decoder_lr = cfg.decoder_lr,\n",
    "        weight_decay = cfg.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr = cfg.encoder_lr, \n",
    "        eps = cfg.eps, \n",
    "        betas = cfg.betas\n",
    "    )\n",
    "    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n",
    "    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "    # Scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps = num_warmup_steps, \n",
    "        num_training_steps = num_train_steps, \n",
    "        num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    # Training & Validation loop\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "    best_score = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        start_time = time.time()\n",
    "        # Train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "        # Compute f2_score\n",
    "        score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'predictions': predictions}, \n",
    "                f\"{cfg.model.replace('/', '-')}_fold{fold}_rerank_exp19.pth\"\n",
    "                )\n",
    "            val_predictions = predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Get best threshold\n",
    "    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "    print(f'Our CV score is {best_score} using a threshold of {best_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444dea5b-f4bd-45af-8667-09ad54ea2162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c76d134796349c2b03035d389edcaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3122756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 172\n"
     ]
    }
   ],
   "source": [
    "# Seed everything\n",
    "seed_everything(CFG)\n",
    "# Read data\n",
    "train, correlations = read_data(CFG)\n",
    "# Get max length\n",
    "get_max_length(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de04015-83a0-4bd7-8e09-132e5a67899c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Epoch: [1][0/19590] Elapsed 0m 1s (remain 492m 19s) Loss: 0.7053(0.7053) Grad: 1.3968  LR: 0.00000000  \n",
      "Epoch: [1][500/19590] Elapsed 1m 11s (remain 45m 26s) Loss: 0.3290(0.6013) Grad: 1.3761  LR: 0.00000051  \n",
      "Epoch: [1][1000/19590] Elapsed 2m 21s (remain 43m 54s) Loss: 0.2337(0.4539) Grad: 0.4571  LR: 0.00000102  \n",
      "Epoch: [1][1500/19590] Elapsed 3m 32s (remain 42m 43s) Loss: 0.3381(0.3995) Grad: 0.9887  LR: 0.00000153  \n",
      "Epoch: [1][2000/19590] Elapsed 4m 43s (remain 41m 30s) Loss: 0.2163(0.3699) Grad: 0.8752  LR: 0.00000204  \n",
      "Epoch: [1][2500/19590] Elapsed 5m 52s (remain 40m 9s) Loss: 0.3841(0.3512) Grad: 2.2373  LR: 0.00000255  \n",
      "Epoch: [1][3000/19590] Elapsed 7m 3s (remain 38m 59s) Loss: 0.2251(0.3373) Grad: 1.1439  LR: 0.00000306  \n",
      "Epoch: [1][3500/19590] Elapsed 8m 13s (remain 37m 46s) Loss: 0.3730(0.3272) Grad: 2.6632  LR: 0.00000357  \n",
      "Epoch: [1][4000/19590] Elapsed 9m 24s (remain 36m 38s) Loss: 0.2073(0.3184) Grad: 2.3759  LR: 0.00000408  \n",
      "Epoch: [1][4500/19590] Elapsed 10m 33s (remain 35m 24s) Loss: 0.2279(0.3107) Grad: 2.6129  LR: 0.00000460  \n",
      "Epoch: [1][5000/19590] Elapsed 11m 44s (remain 34m 14s) Loss: 0.2645(0.3045) Grad: 1.4015  LR: 0.00000511  \n",
      "Epoch: [1][5500/19590] Elapsed 12m 54s (remain 33m 3s) Loss: 0.1905(0.2996) Grad: 1.5399  LR: 0.00000562  \n",
      "Epoch: [1][6000/19590] Elapsed 14m 4s (remain 31m 51s) Loss: 0.1773(0.2949) Grad: 1.5039  LR: 0.00000613  \n",
      "Epoch: [1][6500/19590] Elapsed 15m 13s (remain 30m 39s) Loss: 0.3245(0.2908) Grad: 2.5840  LR: 0.00000664  \n",
      "Epoch: [1][7000/19590] Elapsed 16m 24s (remain 29m 29s) Loss: 0.2111(0.2866) Grad: 1.3981  LR: 0.00000715  \n",
      "Epoch: [1][7500/19590] Elapsed 17m 34s (remain 28m 19s) Loss: 0.2411(0.2833) Grad: 1.5270  LR: 0.00000766  \n",
      "Epoch: [1][8000/19590] Elapsed 18m 44s (remain 27m 8s) Loss: 0.2071(0.2803) Grad: 1.6393  LR: 0.00000817  \n",
      "Epoch: [1][8500/19590] Elapsed 19m 55s (remain 25m 58s) Loss: 0.2160(0.2775) Grad: 1.2103  LR: 0.00000868  \n",
      "Epoch: [1][9000/19590] Elapsed 21m 4s (remain 24m 47s) Loss: 0.2923(0.2749) Grad: 1.4763  LR: 0.00000919  \n",
      "Epoch: [1][9500/19590] Elapsed 22m 14s (remain 23m 36s) Loss: 0.1947(0.2725) Grad: 0.9564  LR: 0.00000970  \n",
      "Epoch: [1][10000/19590] Elapsed 23m 24s (remain 22m 26s) Loss: 0.3476(0.2702) Grad: 2.0673  LR: 0.00001000  \n",
      "Epoch: [1][10500/19590] Elapsed 24m 33s (remain 21m 15s) Loss: 0.1886(0.2682) Grad: 1.7051  LR: 0.00001000  \n",
      "Epoch: [1][11000/19590] Elapsed 25m 44s (remain 20m 5s) Loss: 0.1957(0.2660) Grad: 1.5147  LR: 0.00001000  \n",
      "Epoch: [1][11500/19590] Elapsed 26m 54s (remain 18m 55s) Loss: 0.1762(0.2642) Grad: 1.3617  LR: 0.00000999  \n",
      "Epoch: [1][12000/19590] Elapsed 28m 4s (remain 17m 45s) Loss: 0.1334(0.2623) Grad: 1.0422  LR: 0.00000998  \n",
      "Epoch: [1][12500/19590] Elapsed 29m 14s (remain 16m 34s) Loss: 0.2353(0.2604) Grad: 1.8814  LR: 0.00000998  \n",
      "Epoch: [1][13000/19590] Elapsed 30m 24s (remain 15m 24s) Loss: 0.1747(0.2587) Grad: 1.3369  LR: 0.00000997  \n",
      "Epoch: [1][13500/19590] Elapsed 31m 34s (remain 14m 14s) Loss: 0.3512(0.2572) Grad: 2.0815  LR: 0.00000996  \n",
      "Epoch: [1][14000/19590] Elapsed 32m 44s (remain 13m 4s) Loss: 0.1522(0.2556) Grad: 1.4640  LR: 0.00000994  \n",
      "Epoch: [1][14500/19590] Elapsed 33m 55s (remain 11m 54s) Loss: 0.2394(0.2540) Grad: 1.4314  LR: 0.00000993  \n",
      "Epoch: [1][15000/19590] Elapsed 35m 5s (remain 10m 44s) Loss: 0.2412(0.2525) Grad: 2.0400  LR: 0.00000991  \n",
      "Epoch: [1][15500/19590] Elapsed 36m 15s (remain 9m 33s) Loss: 0.1885(0.2510) Grad: 1.5998  LR: 0.00000990  \n",
      "Epoch: [1][16000/19590] Elapsed 37m 25s (remain 8m 23s) Loss: 0.2842(0.2496) Grad: 1.5792  LR: 0.00000988  \n",
      "Epoch: [1][16500/19590] Elapsed 38m 35s (remain 7m 13s) Loss: 0.1700(0.2483) Grad: 1.2046  LR: 0.00000986  \n",
      "Epoch: [1][17000/19590] Elapsed 39m 45s (remain 6m 3s) Loss: 0.1315(0.2471) Grad: 1.2203  LR: 0.00000984  \n",
      "Epoch: [1][17500/19590] Elapsed 40m 55s (remain 4m 53s) Loss: 0.2246(0.2459) Grad: 1.6212  LR: 0.00000981  \n",
      "Epoch: [1][18000/19590] Elapsed 42m 5s (remain 3m 42s) Loss: 0.1793(0.2446) Grad: 1.8935  LR: 0.00000979  \n",
      "Epoch: [1][18500/19590] Elapsed 43m 15s (remain 2m 32s) Loss: 0.1809(0.2435) Grad: 1.6044  LR: 0.00000976  \n",
      "Epoch: [1][19000/19590] Elapsed 44m 25s (remain 1m 22s) Loss: 0.1091(0.2424) Grad: 1.7935  LR: 0.00000973  \n",
      "Epoch: [1][19500/19590] Elapsed 45m 35s (remain 0m 12s) Loss: 0.2153(0.2413) Grad: 1.4462  LR: 0.00000970  \n",
      "Epoch: [1][19589/19590] Elapsed 45m 47s (remain 0m 0s) Loss: 0.1387(0.2411) Grad: 0.9820  LR: 0.00000970  \n",
      "EVAL: [0/4807] Elapsed 0m 0s (remain 74m 47s) Loss: 0.2289(0.2289) \n",
      "EVAL: [500/4807] Elapsed 0m 18s (remain 2m 38s) Loss: 0.1778(0.1501) \n",
      "EVAL: [1000/4807] Elapsed 0m 34s (remain 2m 13s) Loss: 0.1480(0.1253) \n",
      "EVAL: [1500/4807] Elapsed 0m 54s (remain 1m 59s) Loss: 0.1517(0.1437) \n",
      "EVAL: [2000/4807] Elapsed 1m 14s (remain 1m 44s) Loss: 0.2372(0.1528) \n",
      "EVAL: [2500/4807] Elapsed 1m 35s (remain 1m 27s) Loss: 0.0669(0.1551) \n",
      "EVAL: [3000/4807] Elapsed 1m 56s (remain 1m 10s) Loss: 0.1929(0.1585) \n",
      "EVAL: [3500/4807] Elapsed 2m 18s (remain 0m 51s) Loss: 0.2543(0.1606) \n",
      "EVAL: [4000/4807] Elapsed 2m 41s (remain 0m 32s) Loss: 0.2174(0.1625) \n",
      "EVAL: [4500/4807] Elapsed 3m 6s (remain 0m 12s) Loss: 0.2139(0.1631) \n",
      "EVAL: [4806/4807] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0204(0.1624) \n",
      "Epoch 1 - avg_train_loss: 0.2411  avg_val_loss: 0.1624  time: 3005s\n",
      "Epoch 1 - Score: 0.4024 - Threshold: 0.11000\n",
      "Epoch 1 - Save Best Score: 0.4024 Model\n",
      "Epoch: [2][0/19590] Elapsed 0m 1s (remain 365m 32s) Loss: 0.1058(0.1058) Grad: 1.3575  LR: 0.00000970  \n",
      "Epoch: [2][500/19590] Elapsed 1m 10s (remain 45m 5s) Loss: 0.1019(0.1871) Grad: 1.5066  LR: 0.00000967  \n",
      "Epoch: [2][1000/19590] Elapsed 2m 21s (remain 43m 51s) Loss: 0.2843(0.1879) Grad: 2.2920  LR: 0.00000963  \n",
      "Epoch: [2][1500/19590] Elapsed 3m 31s (remain 42m 27s) Loss: 0.1778(0.1866) Grad: 2.2685  LR: 0.00000960  \n",
      "Epoch: [2][2000/19590] Elapsed 4m 41s (remain 41m 12s) Loss: 0.1898(0.1871) Grad: 1.9709  LR: 0.00000956  \n",
      "Epoch: [2][2500/19590] Elapsed 5m 51s (remain 40m 1s) Loss: 0.1770(0.1873) Grad: 1.7094  LR: 0.00000953  \n",
      "Epoch: [2][3000/19590] Elapsed 7m 1s (remain 38m 52s) Loss: 0.1232(0.1872) Grad: 1.1083  LR: 0.00000949  \n",
      "Epoch: [2][3500/19590] Elapsed 8m 12s (remain 37m 41s) Loss: 0.2358(0.1869) Grad: 2.0738  LR: 0.00000945  \n",
      "Epoch: [2][4000/19590] Elapsed 9m 22s (remain 36m 29s) Loss: 0.1781(0.1871) Grad: 1.9145  LR: 0.00000941  \n",
      "Epoch: [2][4500/19590] Elapsed 10m 32s (remain 35m 20s) Loss: 0.1696(0.1869) Grad: 2.0373  LR: 0.00000937  \n",
      "Epoch: [2][5000/19590] Elapsed 11m 42s (remain 34m 9s) Loss: 0.1624(0.1867) Grad: 2.1282  LR: 0.00000932  \n",
      "Epoch: [2][5500/19590] Elapsed 12m 52s (remain 32m 58s) Loss: 0.1572(0.1865) Grad: 1.6923  LR: 0.00000928  \n",
      "Epoch: [2][6000/19590] Elapsed 14m 2s (remain 31m 47s) Loss: 0.2346(0.1862) Grad: 2.4345  LR: 0.00000923  \n",
      "Epoch: [2][6500/19590] Elapsed 15m 12s (remain 30m 36s) Loss: 0.2529(0.1860) Grad: 2.0622  LR: 0.00000918  \n",
      "Epoch: [2][7000/19590] Elapsed 16m 22s (remain 29m 26s) Loss: 0.1563(0.1856) Grad: 1.5006  LR: 0.00000913  \n",
      "Epoch: [2][7500/19590] Elapsed 17m 32s (remain 28m 16s) Loss: 0.1580(0.1854) Grad: 1.6388  LR: 0.00000908  \n",
      "Epoch: [2][8000/19590] Elapsed 18m 42s (remain 27m 6s) Loss: 0.2192(0.1852) Grad: 2.2008  LR: 0.00000903  \n",
      "Epoch: [2][8500/19590] Elapsed 19m 51s (remain 25m 54s) Loss: 0.1812(0.1850) Grad: 1.8257  LR: 0.00000897  \n",
      "Epoch: [2][9000/19590] Elapsed 21m 2s (remain 24m 45s) Loss: 0.1958(0.1847) Grad: 1.9641  LR: 0.00000892  \n",
      "Epoch: [2][9500/19590] Elapsed 22m 12s (remain 23m 34s) Loss: 0.1686(0.1845) Grad: 2.5189  LR: 0.00000886  \n",
      "Epoch: [2][10000/19590] Elapsed 23m 21s (remain 22m 24s) Loss: 0.0806(0.1841) Grad: 1.6230  LR: 0.00000881  \n",
      "Epoch: [2][10500/19590] Elapsed 24m 32s (remain 21m 14s) Loss: 0.1590(0.1839) Grad: 2.3654  LR: 0.00000875  \n",
      "Epoch: [2][11000/19590] Elapsed 25m 42s (remain 20m 4s) Loss: 0.1054(0.1838) Grad: 1.9213  LR: 0.00000869  \n",
      "Epoch: [2][11500/19590] Elapsed 26m 53s (remain 18m 54s) Loss: 0.1484(0.1836) Grad: 1.5992  LR: 0.00000863  \n",
      "Epoch: [2][12000/19590] Elapsed 28m 2s (remain 17m 44s) Loss: 0.2133(0.1833) Grad: 2.0711  LR: 0.00000857  \n",
      "Epoch: [2][12500/19590] Elapsed 29m 13s (remain 16m 34s) Loss: 0.1984(0.1829) Grad: 2.0690  LR: 0.00000850  \n",
      "Epoch: [2][13000/19590] Elapsed 30m 22s (remain 15m 23s) Loss: 0.1599(0.1826) Grad: 2.0643  LR: 0.00000844  \n",
      "Epoch: [2][13500/19590] Elapsed 31m 32s (remain 14m 13s) Loss: 0.0850(0.1822) Grad: 1.2055  LR: 0.00000837  \n",
      "Epoch: [2][14000/19590] Elapsed 32m 43s (remain 13m 3s) Loss: 0.1282(0.1820) Grad: 1.8250  LR: 0.00000831  \n",
      "Epoch: [2][14500/19590] Elapsed 33m 53s (remain 11m 53s) Loss: 0.1598(0.1817) Grad: 1.9911  LR: 0.00000824  \n",
      "Epoch: [2][15000/19590] Elapsed 35m 3s (remain 10m 43s) Loss: 0.1081(0.1813) Grad: 1.5542  LR: 0.00000817  \n",
      "Epoch: [2][15500/19590] Elapsed 36m 13s (remain 9m 33s) Loss: 0.2139(0.1810) Grad: 2.6118  LR: 0.00000810  \n",
      "Epoch: [2][16000/19590] Elapsed 37m 23s (remain 8m 23s) Loss: 0.1871(0.1807) Grad: 2.9423  LR: 0.00000803  \n",
      "Epoch: [2][16500/19590] Elapsed 38m 33s (remain 7m 13s) Loss: 0.1212(0.1804) Grad: 2.4060  LR: 0.00000796  \n",
      "Epoch: [2][17000/19590] Elapsed 39m 43s (remain 6m 2s) Loss: 0.2367(0.1802) Grad: 3.0425  LR: 0.00000789  \n",
      "Epoch: [2][17500/19590] Elapsed 40m 53s (remain 4m 52s) Loss: 0.2365(0.1799) Grad: 2.6307  LR: 0.00000782  \n",
      "Epoch: [2][18000/19590] Elapsed 42m 3s (remain 3m 42s) Loss: 0.1908(0.1797) Grad: 7.6322  LR: 0.00000774  \n",
      "Epoch: [2][18500/19590] Elapsed 43m 12s (remain 2m 32s) Loss: 0.1837(0.1794) Grad: 2.0250  LR: 0.00000767  \n",
      "Epoch: [2][19000/19590] Elapsed 44m 22s (remain 1m 22s) Loss: 0.1645(0.1792) Grad: 3.1861  LR: 0.00000759  \n",
      "Epoch: [2][19500/19590] Elapsed 45m 32s (remain 0m 12s) Loss: 0.1948(0.1789) Grad: 2.6695  LR: 0.00000751  \n",
      "Epoch: [2][19589/19590] Elapsed 45m 44s (remain 0m 0s) Loss: 0.1049(0.1788) Grad: 1.5550  LR: 0.00000750  \n",
      "EVAL: [0/4807] Elapsed 0m 1s (remain 86m 16s) Loss: 0.2044(0.2044) \n",
      "EVAL: [500/4807] Elapsed 0m 18s (remain 2m 39s) Loss: 0.1518(0.1410) \n",
      "EVAL: [1000/4807] Elapsed 0m 35s (remain 2m 13s) Loss: 0.1605(0.1159) \n",
      "EVAL: [1500/4807] Elapsed 0m 54s (remain 1m 59s) Loss: 0.1764(0.1345) \n",
      "EVAL: [2000/4807] Elapsed 1m 14s (remain 1m 44s) Loss: 0.1769(0.1429) \n",
      "EVAL: [2500/4807] Elapsed 1m 35s (remain 1m 27s) Loss: 0.1236(0.1443) \n",
      "EVAL: [3000/4807] Elapsed 1m 56s (remain 1m 10s) Loss: 0.1919(0.1470) \n",
      "EVAL: [3500/4807] Elapsed 2m 18s (remain 0m 51s) Loss: 0.2197(0.1488) \n",
      "EVAL: [4000/4807] Elapsed 2m 42s (remain 0m 32s) Loss: 0.2289(0.1510) \n",
      "EVAL: [4500/4807] Elapsed 3m 6s (remain 0m 12s) Loss: 0.2292(0.1521) \n",
      "EVAL: [4806/4807] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0441(0.1513) \n",
      "Epoch 2 - avg_train_loss: 0.1788  avg_val_loss: 0.1513  time: 3008s\n",
      "Epoch 2 - Score: 0.4413 - Threshold: 0.11000\n",
      "Epoch 2 - Save Best Score: 0.4413 Model\n",
      "Epoch: [3][0/19590] Elapsed 0m 1s (remain 391m 18s) Loss: 0.1172(0.1172) Grad: 2.0621  LR: 0.00000750  \n",
      "Epoch: [3][500/19590] Elapsed 1m 10s (remain 44m 48s) Loss: 0.1322(0.1491) Grad: 1.9050  LR: 0.00000742  \n",
      "Epoch: [3][1000/19590] Elapsed 2m 20s (remain 43m 35s) Loss: 0.1268(0.1497) Grad: 2.0093  LR: 0.00000734  \n",
      "Epoch: [3][1500/19590] Elapsed 3m 31s (remain 42m 26s) Loss: 0.1530(0.1503) Grad: 2.5971  LR: 0.00000726  \n",
      "Epoch: [3][2000/19590] Elapsed 4m 41s (remain 41m 15s) Loss: 0.1138(0.1510) Grad: 1.5986  LR: 0.00000719  \n",
      "Epoch: [3][2500/19590] Elapsed 5m 52s (remain 40m 5s) Loss: 0.1492(0.1509) Grad: 2.2868  LR: 0.00000710  \n",
      "Epoch: [3][3000/19590] Elapsed 7m 2s (remain 38m 54s) Loss: 0.1360(0.1500) Grad: 2.1714  LR: 0.00000702  \n",
      "Epoch: [3][3500/19590] Elapsed 8m 12s (remain 37m 43s) Loss: 0.2083(0.1498) Grad: 3.9797  LR: 0.00000694  \n",
      "Epoch: [3][4000/19590] Elapsed 9m 23s (remain 36m 35s) Loss: 0.1396(0.1495) Grad: 2.4748  LR: 0.00000686  \n",
      "Epoch: [3][4500/19590] Elapsed 10m 33s (remain 35m 25s) Loss: 0.1769(0.1491) Grad: 3.1418  LR: 0.00000678  \n",
      "Epoch: [3][5000/19590] Elapsed 11m 43s (remain 34m 12s) Loss: 0.1168(0.1488) Grad: 2.2912  LR: 0.00000669  \n",
      "Epoch: [3][5500/19590] Elapsed 12m 53s (remain 32m 59s) Loss: 0.1260(0.1487) Grad: 1.8324  LR: 0.00000661  \n",
      "Epoch: [3][6000/19590] Elapsed 14m 3s (remain 31m 49s) Loss: 0.1985(0.1485) Grad: 3.3030  LR: 0.00000652  \n",
      "Epoch: [3][6500/19590] Elapsed 15m 12s (remain 30m 37s) Loss: 0.1706(0.1488) Grad: 2.8788  LR: 0.00000644  \n",
      "Epoch: [3][7000/19590] Elapsed 16m 23s (remain 29m 28s) Loss: 0.1072(0.1486) Grad: 2.3046  LR: 0.00000635  \n",
      "Epoch: [3][7500/19590] Elapsed 17m 33s (remain 28m 17s) Loss: 0.2552(0.1484) Grad: 4.3715  LR: 0.00000627  \n",
      "Epoch: [3][8000/19590] Elapsed 18m 43s (remain 27m 6s) Loss: 0.1568(0.1483) Grad: 2.8920  LR: 0.00000618  \n",
      "Epoch: [3][8500/19590] Elapsed 19m 52s (remain 25m 55s) Loss: 0.0713(0.1481) Grad: 1.7295  LR: 0.00000609  \n",
      "Epoch: [3][9000/19590] Elapsed 21m 2s (remain 24m 45s) Loss: 0.1362(0.1478) Grad: 2.2186  LR: 0.00000601  \n",
      "Epoch: [3][9500/19590] Elapsed 22m 12s (remain 23m 35s) Loss: 0.1114(0.1475) Grad: 1.8110  LR: 0.00000592  \n",
      "Epoch: [3][10000/19590] Elapsed 23m 22s (remain 22m 24s) Loss: 0.0727(0.1474) Grad: 1.2363  LR: 0.00000583  \n",
      "Epoch: [3][10500/19590] Elapsed 24m 33s (remain 21m 15s) Loss: 0.1354(0.1474) Grad: 2.2940  LR: 0.00000574  \n",
      "Epoch: [3][11000/19590] Elapsed 25m 42s (remain 20m 4s) Loss: 0.1172(0.1473) Grad: 1.9756  LR: 0.00000566  \n",
      "Epoch: [3][11500/19590] Elapsed 26m 53s (remain 18m 54s) Loss: 0.1170(0.1471) Grad: 2.1692  LR: 0.00000557  \n",
      "Epoch: [3][12000/19590] Elapsed 28m 3s (remain 17m 44s) Loss: 0.0898(0.1470) Grad: 2.6688  LR: 0.00000548  \n",
      "Epoch: [3][12500/19590] Elapsed 29m 12s (remain 16m 33s) Loss: 0.1867(0.1469) Grad: 2.5797  LR: 0.00000539  \n",
      "Epoch: [3][13000/19590] Elapsed 30m 22s (remain 15m 23s) Loss: 0.1214(0.1468) Grad: 4.0284  LR: 0.00000530  \n",
      "Epoch: [3][13500/19590] Elapsed 31m 33s (remain 14m 13s) Loss: 0.1040(0.1467) Grad: 3.0310  LR: 0.00000521  \n",
      "Epoch: [3][14000/19590] Elapsed 32m 42s (remain 13m 3s) Loss: 0.1198(0.1465) Grad: 3.5285  LR: 0.00000512  \n",
      "Epoch: [3][14500/19590] Elapsed 33m 52s (remain 11m 53s) Loss: 0.1438(0.1463) Grad: 2.4684  LR: 0.00000503  \n",
      "Epoch: [3][15000/19590] Elapsed 35m 2s (remain 10m 43s) Loss: 0.2161(0.1462) Grad: 3.9908  LR: 0.00000495  \n",
      "Epoch: [3][15500/19590] Elapsed 36m 11s (remain 9m 32s) Loss: 0.1246(0.1460) Grad: 2.1932  LR: 0.00000486  \n",
      "Epoch: [3][16000/19590] Elapsed 37m 21s (remain 8m 22s) Loss: 0.0946(0.1459) Grad: 2.1512  LR: 0.00000477  \n",
      "Epoch: [3][16500/19590] Elapsed 38m 30s (remain 7m 12s) Loss: 0.1207(0.1458) Grad: 2.1308  LR: 0.00000468  \n",
      "Epoch: [3][17000/19590] Elapsed 39m 40s (remain 6m 2s) Loss: 0.1710(0.1456) Grad: 2.8740  LR: 0.00000459  \n",
      "Epoch: [3][17500/19590] Elapsed 40m 50s (remain 4m 52s) Loss: 0.2046(0.1455) Grad: 3.6914  LR: 0.00000450  \n",
      "Epoch: [3][18000/19590] Elapsed 42m 0s (remain 3m 42s) Loss: 0.0996(0.1453) Grad: 2.3709  LR: 0.00000441  \n",
      "Epoch: [3][18500/19590] Elapsed 43m 11s (remain 2m 32s) Loss: 0.1739(0.1451) Grad: 3.3591  LR: 0.00000432  \n",
      "Epoch: [3][19000/19590] Elapsed 44m 20s (remain 1m 22s) Loss: 0.1520(0.1449) Grad: 3.5110  LR: 0.00000424  \n",
      "Epoch: [3][19500/19590] Elapsed 45m 31s (remain 0m 12s) Loss: 0.2303(0.1448) Grad: 4.2289  LR: 0.00000415  \n",
      "Epoch: [3][19589/19590] Elapsed 45m 44s (remain 0m 0s) Loss: 0.1432(0.1447) Grad: 3.0369  LR: 0.00000413  \n",
      "EVAL: [0/4807] Elapsed 0m 0s (remain 73m 50s) Loss: 0.1446(0.1446) \n",
      "EVAL: [500/4807] Elapsed 0m 18s (remain 2m 38s) Loss: 0.1645(0.1383) \n",
      "EVAL: [1000/4807] Elapsed 0m 34s (remain 2m 12s) Loss: 0.1391(0.1156) \n",
      "EVAL: [1500/4807] Elapsed 0m 54s (remain 1m 59s) Loss: 0.2229(0.1340) \n",
      "EVAL: [2000/4807] Elapsed 1m 14s (remain 1m 43s) Loss: 0.1376(0.1414) \n",
      "EVAL: [2500/4807] Elapsed 1m 34s (remain 1m 27s) Loss: 0.1407(0.1425) \n",
      "EVAL: [3000/4807] Elapsed 1m 56s (remain 1m 10s) Loss: 0.1740(0.1454) \n",
      "EVAL: [3500/4807] Elapsed 2m 18s (remain 0m 51s) Loss: 0.2135(0.1473) \n",
      "EVAL: [4000/4807] Elapsed 2m 41s (remain 0m 32s) Loss: 0.1840(0.1488) \n",
      "EVAL: [4500/4807] Elapsed 3m 6s (remain 0m 12s) Loss: 0.2408(0.1493) \n",
      "EVAL: [4806/4807] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0344(0.1483) \n",
      "Epoch 3 - avg_train_loss: 0.1447  avg_val_loss: 0.1483  time: 3007s\n",
      "Epoch 3 - Score: 0.4606 - Threshold: 0.07000\n",
      "Epoch 3 - Save Best Score: 0.4606 Model\n",
      "Epoch: [4][0/19590] Elapsed 0m 1s (remain 362m 6s) Loss: 0.1095(0.1095) Grad: 2.3045  LR: 0.00000413  \n",
      "Epoch: [4][500/19590] Elapsed 1m 11s (remain 45m 8s) Loss: 0.1512(0.1211) Grad: 3.0851  LR: 0.00000404  \n",
      "Epoch: [4][1000/19590] Elapsed 2m 21s (remain 43m 42s) Loss: 0.0934(0.1182) Grad: 2.3148  LR: 0.00000396  \n",
      "Epoch: [4][1500/19590] Elapsed 3m 31s (remain 42m 26s) Loss: 0.0521(0.1183) Grad: 1.8730  LR: 0.00000387  \n",
      "Epoch: [4][2000/19590] Elapsed 4m 41s (remain 41m 12s) Loss: 0.2376(0.1190) Grad: 6.4189  LR: 0.00000378  \n",
      "Epoch: [4][2500/19590] Elapsed 5m 50s (remain 39m 57s) Loss: 0.2405(0.1186) Grad: 4.5904  LR: 0.00000370  \n",
      "Epoch: [4][3000/19590] Elapsed 7m 1s (remain 38m 48s) Loss: 0.1181(0.1186) Grad: 3.8704  LR: 0.00000361  \n",
      "Epoch: [4][3500/19590] Elapsed 8m 10s (remain 37m 35s) Loss: 0.1088(0.1186) Grad: 3.4521  LR: 0.00000353  \n",
      "Epoch: [4][4000/19590] Elapsed 9m 20s (remain 36m 24s) Loss: 0.0948(0.1190) Grad: 3.3152  LR: 0.00000344  \n",
      "Epoch: [4][4500/19590] Elapsed 10m 32s (remain 35m 19s) Loss: 0.0838(0.1191) Grad: 3.3444  LR: 0.00000336  \n",
      "Epoch: [4][5000/19590] Elapsed 11m 42s (remain 34m 9s) Loss: 0.0815(0.1190) Grad: 2.4173  LR: 0.00000327  \n",
      "Epoch: [4][5500/19590] Elapsed 12m 52s (remain 32m 58s) Loss: 0.0721(0.1190) Grad: 2.9678  LR: 0.00000319  \n",
      "Epoch: [4][6000/19590] Elapsed 14m 2s (remain 31m 47s) Loss: 0.1293(0.1189) Grad: 4.0638  LR: 0.00000311  \n",
      "Epoch: [4][6500/19590] Elapsed 15m 12s (remain 30m 37s) Loss: 0.0973(0.1188) Grad: 3.1990  LR: 0.00000302  \n",
      "Epoch: [4][7000/19590] Elapsed 16m 22s (remain 29m 26s) Loss: 0.1726(0.1187) Grad: 4.8514  LR: 0.00000294  \n",
      "Epoch: [4][7500/19590] Elapsed 17m 32s (remain 28m 15s) Loss: 0.0926(0.1187) Grad: 2.6905  LR: 0.00000286  \n",
      "Epoch: [4][8000/19590] Elapsed 18m 42s (remain 27m 5s) Loss: 0.0915(0.1185) Grad: 2.2293  LR: 0.00000278  \n",
      "Epoch: [4][8500/19590] Elapsed 19m 52s (remain 25m 55s) Loss: 0.1421(0.1185) Grad: 5.1579  LR: 0.00000270  \n",
      "Epoch: [4][9000/19590] Elapsed 21m 2s (remain 24m 45s) Loss: 0.1568(0.1186) Grad: 4.4597  LR: 0.00000262  \n",
      "Epoch: [4][9500/19590] Elapsed 22m 12s (remain 23m 35s) Loss: 0.1293(0.1186) Grad: 5.3484  LR: 0.00000255  \n",
      "Epoch: [4][10000/19590] Elapsed 23m 22s (remain 22m 24s) Loss: 0.0765(0.1185) Grad: 2.6755  LR: 0.00000247  \n",
      "Epoch: [4][10500/19590] Elapsed 24m 32s (remain 21m 14s) Loss: 0.1284(0.1184) Grad: 4.3906  LR: 0.00000239  \n",
      "Epoch: [4][11000/19590] Elapsed 25m 43s (remain 20m 4s) Loss: 0.1867(0.1184) Grad: 8.3629  LR: 0.00000232  \n",
      "Epoch: [4][11500/19590] Elapsed 26m 53s (remain 18m 54s) Loss: 0.1225(0.1183) Grad: 4.8475  LR: 0.00000224  \n",
      "Epoch: [4][12000/19590] Elapsed 28m 3s (remain 17m 44s) Loss: 0.1664(0.1182) Grad: 4.0148  LR: 0.00000217  \n",
      "Epoch: [4][12500/19590] Elapsed 29m 13s (remain 16m 34s) Loss: 0.0704(0.1181) Grad: 2.5396  LR: 0.00000209  \n",
      "Epoch: [4][13000/19590] Elapsed 30m 23s (remain 15m 24s) Loss: 0.0848(0.1180) Grad: 3.6594  LR: 0.00000202  \n",
      "Epoch: [4][13500/19590] Elapsed 31m 34s (remain 14m 14s) Loss: 0.0725(0.1178) Grad: 2.5850  LR: 0.00000195  \n",
      "Epoch: [4][14000/19590] Elapsed 32m 44s (remain 13m 4s) Loss: 0.1563(0.1176) Grad: 3.5443  LR: 0.00000188  \n",
      "Epoch: [4][14500/19590] Elapsed 33m 54s (remain 11m 54s) Loss: 0.1251(0.1175) Grad: 3.7758  LR: 0.00000181  \n",
      "Epoch: [4][15000/19590] Elapsed 35m 5s (remain 10m 43s) Loss: 0.0546(0.1174) Grad: 1.9604  LR: 0.00000174  \n",
      "Epoch: [4][15500/19590] Elapsed 36m 14s (remain 9m 33s) Loss: 0.0996(0.1173) Grad: 3.2734  LR: 0.00000168  \n",
      "Epoch: [4][16000/19590] Elapsed 37m 24s (remain 8m 23s) Loss: 0.1702(0.1173) Grad: 5.3748  LR: 0.00000161  \n",
      "Epoch: [4][16500/19590] Elapsed 38m 34s (remain 7m 13s) Loss: 0.0821(0.1173) Grad: 3.2585  LR: 0.00000155  \n",
      "Epoch: [4][17000/19590] Elapsed 39m 44s (remain 6m 3s) Loss: 0.1053(0.1172) Grad: 5.5465  LR: 0.00000148  \n",
      "Epoch: [4][17500/19590] Elapsed 40m 55s (remain 4m 53s) Loss: 0.0822(0.1171) Grad: 3.2509  LR: 0.00000142  \n",
      "Epoch: [4][18000/19590] Elapsed 42m 5s (remain 3m 42s) Loss: 0.1287(0.1170) Grad: 3.5119  LR: 0.00000136  \n",
      "Epoch: [4][18500/19590] Elapsed 43m 14s (remain 2m 32s) Loss: 0.0948(0.1169) Grad: 3.1613  LR: 0.00000130  \n",
      "Epoch: [4][19000/19590] Elapsed 44m 25s (remain 1m 22s) Loss: 0.0788(0.1168) Grad: 3.0589  LR: 0.00000124  \n",
      "Epoch: [4][19500/19590] Elapsed 45m 35s (remain 0m 12s) Loss: 0.1340(0.1167) Grad: 3.8434  LR: 0.00000118  \n",
      "Epoch: [4][19589/19590] Elapsed 45m 47s (remain 0m 0s) Loss: 0.2271(0.1167) Grad: 4.3504  LR: 0.00000117  \n",
      "EVAL: [0/4807] Elapsed 0m 0s (remain 78m 25s) Loss: 0.1365(0.1365) \n",
      "EVAL: [500/4807] Elapsed 0m 18s (remain 2m 39s) Loss: 0.1619(0.1482) \n",
      "EVAL: [1000/4807] Elapsed 0m 35s (remain 2m 13s) Loss: 0.1378(0.1239) \n",
      "EVAL: [1500/4807] Elapsed 0m 54s (remain 1m 59s) Loss: 0.2568(0.1456) \n",
      "EVAL: [2000/4807] Elapsed 1m 14s (remain 1m 44s) Loss: 0.1357(0.1540) \n",
      "EVAL: [2500/4807] Elapsed 1m 35s (remain 1m 27s) Loss: 0.2592(0.1557) \n",
      "EVAL: [3000/4807] Elapsed 1m 56s (remain 1m 10s) Loss: 0.1739(0.1604) \n",
      "EVAL: [3500/4807] Elapsed 2m 18s (remain 0m 51s) Loss: 0.1840(0.1624) \n",
      "EVAL: [4000/4807] Elapsed 2m 41s (remain 0m 32s) Loss: 0.1984(0.1636) \n",
      "EVAL: [4500/4807] Elapsed 3m 6s (remain 0m 12s) Loss: 0.2380(0.1635) \n",
      "EVAL: [4806/4807] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0323(0.1624) \n",
      "Epoch 4 - avg_train_loss: 0.1167  avg_val_loss: 0.1624  time: 3015s\n",
      "Epoch 4 - Score: 0.4704 - Threshold: 0.08000\n",
      "Epoch 4 - Save Best Score: 0.4704 Model\n",
      "Epoch: [5][0/19590] Elapsed 0m 1s (remain 410m 55s) Loss: 0.0488(0.0488) Grad: 2.4108  LR: 0.00000117  \n",
      "Epoch: [5][500/19590] Elapsed 1m 10s (remain 44m 52s) Loss: 0.0992(0.1012) Grad: 3.5788  LR: 0.00000111  \n",
      "Epoch: [5][1000/19590] Elapsed 2m 20s (remain 43m 27s) Loss: 0.1299(0.1004) Grad: 3.5419  LR: 0.00000106  \n",
      "Epoch: [5][1500/19590] Elapsed 3m 30s (remain 42m 17s) Loss: 0.0649(0.0999) Grad: 2.9876  LR: 0.00000100  \n",
      "Epoch: [5][2000/19590] Elapsed 4m 40s (remain 41m 3s) Loss: 0.1085(0.0993) Grad: 3.3950  LR: 0.00000095  \n",
      "Epoch: [5][2500/19590] Elapsed 5m 50s (remain 39m 54s) Loss: 0.0531(0.0989) Grad: 3.3549  LR: 0.00000090  \n",
      "Epoch: [5][3000/19590] Elapsed 7m 0s (remain 38m 42s) Loss: 0.1543(0.0991) Grad: 4.3573  LR: 0.00000085  \n",
      "Epoch: [5][3500/19590] Elapsed 8m 10s (remain 37m 34s) Loss: 0.0915(0.0989) Grad: 2.4798  LR: 0.00000080  \n",
      "Epoch: [5][4000/19590] Elapsed 9m 20s (remain 36m 25s) Loss: 0.1448(0.0991) Grad: 4.9441  LR: 0.00000075  \n",
      "Epoch: [5][4500/19590] Elapsed 10m 31s (remain 35m 17s) Loss: 0.1303(0.0991) Grad: 4.5779  LR: 0.00000071  \n",
      "Epoch: [5][5000/19590] Elapsed 11m 41s (remain 34m 7s) Loss: 0.1868(0.0993) Grad: 5.3789  LR: 0.00000066  \n",
      "Epoch: [5][5500/19590] Elapsed 12m 51s (remain 32m 55s) Loss: 0.1160(0.0993) Grad: 3.8099  LR: 0.00000062  \n",
      "Epoch: [5][6000/19590] Elapsed 14m 1s (remain 31m 44s) Loss: 0.1216(0.0991) Grad: 4.7921  LR: 0.00000058  \n",
      "Epoch: [5][6500/19590] Elapsed 15m 11s (remain 30m 34s) Loss: 0.1128(0.0990) Grad: 4.4413  LR: 0.00000053  \n",
      "Epoch: [5][7000/19590] Elapsed 16m 21s (remain 29m 24s) Loss: 0.0967(0.0991) Grad: 3.7934  LR: 0.00000049  \n",
      "Epoch: [5][7500/19590] Elapsed 17m 31s (remain 28m 14s) Loss: 0.0781(0.0990) Grad: 3.2179  LR: 0.00000046  \n",
      "Epoch: [5][8000/19590] Elapsed 18m 41s (remain 27m 3s) Loss: 0.0615(0.0990) Grad: 2.6307  LR: 0.00000042  \n",
      "Epoch: [5][8500/19590] Elapsed 19m 51s (remain 25m 54s) Loss: 0.0807(0.0990) Grad: 3.6167  LR: 0.00000039  \n",
      "Epoch: [5][9000/19590] Elapsed 21m 1s (remain 24m 43s) Loss: 0.0727(0.0992) Grad: 3.9554  LR: 0.00000035  \n",
      "Epoch: [5][9500/19590] Elapsed 22m 11s (remain 23m 33s) Loss: 0.0982(0.0990) Grad: 5.5951  LR: 0.00000032  \n",
      "Epoch: [5][10000/19590] Elapsed 23m 21s (remain 22m 23s) Loss: 0.0896(0.0991) Grad: 3.0733  LR: 0.00000029  \n",
      "Epoch: [5][10500/19590] Elapsed 24m 31s (remain 21m 13s) Loss: 0.0841(0.0991) Grad: 2.8819  LR: 0.00000026  \n",
      "Epoch: [5][11000/19590] Elapsed 25m 41s (remain 20m 3s) Loss: 0.1413(0.0991) Grad: 4.3084  LR: 0.00000023  \n",
      "Epoch: [5][11500/19590] Elapsed 26m 50s (remain 18m 52s) Loss: 0.1611(0.0991) Grad: 5.4061  LR: 0.00000021  \n",
      "Epoch: [5][12000/19590] Elapsed 28m 1s (remain 17m 43s) Loss: 0.1716(0.0989) Grad: 7.4700  LR: 0.00000018  \n",
      "Epoch: [5][12500/19590] Elapsed 29m 11s (remain 16m 33s) Loss: 0.0839(0.0990) Grad: 5.7198  LR: 0.00000016  \n",
      "Epoch: [5][13000/19590] Elapsed 30m 21s (remain 15m 22s) Loss: 0.0894(0.0990) Grad: 3.4605  LR: 0.00000014  \n",
      "Epoch: [5][13500/19590] Elapsed 31m 30s (remain 14m 12s) Loss: 0.0457(0.0990) Grad: 1.8665  LR: 0.00000012  \n",
      "Epoch: [5][14000/19590] Elapsed 32m 40s (remain 13m 2s) Loss: 0.1618(0.0990) Grad: 5.4687  LR: 0.00000010  \n",
      "Epoch: [5][14500/19590] Elapsed 33m 51s (remain 11m 52s) Loss: 0.1261(0.0989) Grad: 4.4816  LR: 0.00000008  \n",
      "Epoch: [5][15000/19590] Elapsed 35m 1s (remain 10m 42s) Loss: 0.1030(0.0990) Grad: 3.0572  LR: 0.00000007  \n",
      "Epoch: [5][15500/19590] Elapsed 36m 12s (remain 9m 33s) Loss: 0.1404(0.0990) Grad: 4.5141  LR: 0.00000005  \n",
      "Epoch: [5][16000/19590] Elapsed 37m 22s (remain 8m 23s) Loss: 0.0937(0.0990) Grad: 4.8123  LR: 0.00000004  \n",
      "Epoch: [5][16500/19590] Elapsed 38m 32s (remain 7m 12s) Loss: 0.1474(0.0990) Grad: 4.0623  LR: 0.00000003  \n",
      "Epoch: [5][17000/19590] Elapsed 39m 43s (remain 6m 2s) Loss: 0.0777(0.0990) Grad: 5.7177  LR: 0.00000002  \n",
      "Epoch: [5][17500/19590] Elapsed 40m 53s (remain 4m 52s) Loss: 0.0567(0.0989) Grad: 2.7012  LR: 0.00000001  \n",
      "Epoch: [5][18000/19590] Elapsed 42m 3s (remain 3m 42s) Loss: 0.0723(0.0989) Grad: 4.0345  LR: 0.00000001  \n",
      "Epoch: [5][18500/19590] Elapsed 43m 13s (remain 2m 32s) Loss: 0.0709(0.0989) Grad: 3.5345  LR: 0.00000000  \n",
      "Epoch: [5][19000/19590] Elapsed 44m 24s (remain 1m 22s) Loss: 0.0713(0.0989) Grad: 3.5619  LR: 0.00000000  \n",
      "Epoch: [5][19500/19590] Elapsed 45m 34s (remain 0m 12s) Loss: 0.0602(0.0989) Grad: 2.8189  LR: 0.00000000  \n",
      "Epoch: [5][19589/19590] Elapsed 45m 47s (remain 0m 0s) Loss: 0.0413(0.0989) Grad: 3.3400  LR: 0.00000000  \n",
      "EVAL: [0/4807] Elapsed 0m 0s (remain 70m 30s) Loss: 0.1378(0.1378) \n",
      "EVAL: [500/4807] Elapsed 0m 18s (remain 2m 38s) Loss: 0.1600(0.1545) \n",
      "EVAL: [1000/4807] Elapsed 0m 34s (remain 2m 12s) Loss: 0.1450(0.1289) \n",
      "EVAL: [1500/4807] Elapsed 0m 54s (remain 1m 59s) Loss: 0.2776(0.1538) \n",
      "EVAL: [2000/4807] Elapsed 1m 14s (remain 1m 44s) Loss: 0.1295(0.1629) \n",
      "EVAL: [2500/4807] Elapsed 1m 35s (remain 1m 27s) Loss: 0.2585(0.1646) \n",
      "EVAL: [3000/4807] Elapsed 1m 56s (remain 1m 10s) Loss: 0.1825(0.1697) \n",
      "EVAL: [3500/4807] Elapsed 2m 18s (remain 0m 51s) Loss: 0.1824(0.1721) \n",
      "EVAL: [4000/4807] Elapsed 2m 41s (remain 0m 32s) Loss: 0.1938(0.1732) \n",
      "EVAL: [4500/4807] Elapsed 3m 6s (remain 0m 12s) Loss: 0.2513(0.1729) \n",
      "EVAL: [4806/4807] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0489(0.1718) \n",
      "Epoch 5 - avg_train_loss: 0.0989  avg_val_loss: 0.1718  time: 3015s\n",
      "Epoch 5 - Score: 0.4713 - Threshold: 0.08000\n",
      "Epoch 5 - Save Best Score: 0.4713 Model\n",
      "Our CV score is 0.4713 using a threshold of 0.08\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate one fold\n",
    "train_and_evaluate_one_fold(train, correlations, 0, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ef6e2-a9e5-4820-aab3-d8fe0060bc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
