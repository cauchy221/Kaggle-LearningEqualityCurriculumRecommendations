{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is my notebook for the competition LECR :)\n\nI'll do a simple data analysis, and try models like SBERT and SimCSE.\n\nI'm new to Kaggle. Please leave your comments and I'll make improvements :)","metadata":{}},{"cell_type":"markdown","source":"# Introduction\nThis is where I briefly introduce the data provided by the host.\n\nAnd I will mention some tips for this competition.","metadata":{}},{"cell_type":"markdown","source":"1. GOAL: The goal of this competition is to match content to the specific topic *(WARNING: THERE MIGHT BE MORE THAN ONE CORRELATED CONTENT)*.\n\n\n2. DATASET:\n    * `topics.csv`:  Contains a row for each topic in the dataset.\n    * `content.csv`: Contains a row for each content item in the dataset.\n    * `correlations.csv`: Contains `topic_id` with the correlated `content_ids`.\n    * `sample_submission.csv`: A submission file in the correct format *(WARNING: YOU SHOULD ONLY SUBMIT PREDICTIONS FOR THOSE TOPICS LISTED IN THIS FILE)*.\n\n\n3. EVALUATION:\n    * LECR uses MEAN F2 SCORE for evaluation.\n    * Since it's a CODE COMPETITION, the actual test set contains additional topics and content items. In the public version, the sample test data are drawn from the training set.\n\n\n4. TIPS FROM THE HOST:\n    * **Context matters! Explore the tree**: This is because other nodes in the topic tree may contain more semantic context than the given topic.\n    * **Narrow down by language**: Most of the time the language of a topic will match the language of its correlated content. You can use this feature to narrow down or give priority.\n    * **Focus on aligned and supplemental for performance**: The testing dataset does not contain any topics from `source` channels. So it's important to focus on these kind of data to achieve a better performance.\n    * **Balance the semantics of `title`, `description`, and `text`**: It's important to carefully weight these fields because the semantic information they contain vary across topics and content items.\n    * **Disregard `copyright_holder` for training purposes**: This field is blanked out in the testing data. So don't use it in the training phase.\n    * **Restructure correlations for efficiency**: Considering the efficiency, you may need to reconstruct the correlations in `correlations.csv`.","metadata":{}},{"cell_type":"markdown","source":"# Exploratoty Data Analysis\nThis is where I do some simple data analysis.","metadata":{}},{"cell_type":"markdown","source":"# Modeling\nThis is where I try models like SBERT and SimCSE.\n\nI'll explain why I choose these two models, and provide some examples to illustrate the usage.","metadata":{}},{"cell_type":"markdown","source":"Considering the GOAL of this competition (matching content items to the given topic), this is actually a Semantic Textual Similarity task.\n\n[SBERT](https://www.sbert.net/), or Sentence-BERT is a novel framework for computing sentence/text embeddings for more than 100 languages **(multilingual)**. These embeddings can then be used in tasks like semantic similarity. It's easy to use with the help of Hugging Face. SBERT acutally uses a Siamese Network Structure, which is way **faster** than the usual Cross-Encoder Structure.\n\n[SimCSE](https://github.com/princeton-nlp/SimCSE) adds the idea of contrastive learning to SBERT. It achieves **SOTA** performance on **unsupervised** learning task.","metadata":{}},{"cell_type":"markdown","source":"# Evaluation\nThis is where I explain the F2 SCORE.","metadata":{}},{"cell_type":"markdown","source":"When we evaluate a mdoel, we often use metrics like Precision and Recall. However, it's hard to achieve good results on both scores, so people should trade off between Precision and Recall. That's when people came up with a measurement called the **F-Score**. \n\nThe F-Score is a the harmonic mean of a system's precision and recall values. It can be calculated by the following formula:\n\n$$F\\text-Score = (1+\\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision+Recall}$$","metadata":{}},{"cell_type":"markdown","source":"If we set $\\beta = n$, the $F\\text-Score$ is now the $F_n\\text-Score$. This means that if we set $\\beta = 2$, we'll get the formula of **F2-Score**.","metadata":{}}]}