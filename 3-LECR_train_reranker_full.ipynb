{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109163fc-38f4-4dcc-9711-7ff9e4b02448",
   "metadata": {},
   "source": [
    "# Step 3: Get our reranker\n",
    "\n",
    "Use recalled data from step 2 to continue finetuning our finetuned retriever model in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae9c231-cb2b-4d9a-9251-e5435068b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3774baca-48da-44b1-9010-7e0af6b76404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0dba531-b826-47a2-89bf-6d8165b0929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    print_freq = 2000\n",
    "    num_workers = 4\n",
    "    model = 'autodl-tmp/paraphrase-distilroberta-base-v1-exp21_fold0_epochs10'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    epochs = 10\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 64\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    max_len = 512\n",
    "    n_folds = 5\n",
    "    OUTPUT_DIR = '/root/autodl-tmp/'\n",
    "    seed = 1006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3968020-6e59-4c80-9d15-e9439fffa330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def modity_state_dict(cfg):\n",
    "#     config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "#     config.hidden_dropout = 0.0\n",
    "#     config.hidden_dropout_prob = 0.0\n",
    "#     config.attention_dropout = 0.0\n",
    "#     config.attention_probs_dropout_prob = 0.0\n",
    "#     model = AutoModel.from_pretrained(cfg.model, config=config)\n",
    "#     state = model.state_dict()\n",
    "    \n",
    "#     state['embeddings.position_ids'] = torch.arange(0, cfg.max_len, 1).unsqueeze(dim=0)\n",
    "#     state['embeddings.position_embeddings.weight'] = torch.randn(cfg.max_len, 768)\n",
    "    \n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d622943c-25d3-4441-889d-8e5449883f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc22cdc-a4b7-4d11-9874-bc9dc5176a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6920e72-c8e2-4453-9f4d-dab5619f490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_data():\n",
    "#     train = pd.DataFrame()\n",
    "#     for i in range(1,6):\n",
    "#         print(f'========read file {i}========')\n",
    "#         tmp = pd.read_csv(f'autodl-tmp/recall_data_top100_fold0_exp21_{i}.csv')\n",
    "#         train = pd.concat([train,tmp], axis=0, ignore_index=True)\n",
    "#         train.drop_duplicates(subset=['topics_ids','content_ids'], keep='first', inplace=True, ignore_index=True)\n",
    "#         del tmp\n",
    "#         gc.collect()\n",
    "#     train.to_csv('autodl-tmp/recall_data_top100_fold0_exp21_full.csv')\n",
    "#     return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bd92b5-88a3-4b88-9f0e-c0142755cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_max_length(train, cfg):\n",
    "#     max_length = 0\n",
    "#     for index, row in tqdm(train.iterrows(), total = len(train)):\n",
    "#         length = len(cfg.tokenizer(row['title1']+'[TAC]'+row['title2'], add_special_tokens=True)['input_ids'])\n",
    "#         max_length = max(max_length, length)\n",
    "#     cfg.max_len = max_length + 2 # cls & sep\n",
    "#     print(f\"max_len: {cfg.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb1176f-86d5-47f6-9fcc-063ffb86cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text, cfg):\n",
    "    text_enc = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    for k, v in text_enc.items():\n",
    "        text_enc[k] = torch.tensor(v, dtype = torch.long)\n",
    "        \n",
    "    return text_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f63fd00-6c2f-469a-adb7-02eff245261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.text = df['text'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.text[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390aee51-d10b-448d-9ccc-74a344897e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5c84b0-55b3-416f-b0cf-8dd47a508ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        # modify model architecture\n",
    "        # self.state_dict = modity_state_dict(cfg)\n",
    "        # self.model = AutoModel.from_pretrained(cfg.model, config=self.config, state_dict=self.state_dict)\n",
    "        # ignore\n",
    "        # self.model = AutoModel.from_pretrained(cfg.model, config=self.config, ignore_mismatched_sizes=True)\n",
    "        # normal\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b6ac19-f441-4bcd-b2b8-f0fadae625f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2398a216-a9a3-4b6f-a1e5-9689cf27e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_1e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_10===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9abe6b7c-c675-4531-a697-21529383c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cacfbe93-f3b8-4f53-997e-67869d0512bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(tqdm(valid_loader)):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea7daf1-94da-4a65-a7b0-a46841b4d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.001, 1, 0.001):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acee84b1-e163-49cf-aca5-3b6d62f7ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_one_fold(train, correlations, fold, cfg):\n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "    # Split train & validation\n",
    "    x_train = train[train['fold'] != fold]\n",
    "    x_val = train[train['fold'] == fold]\n",
    "    valid_labels = x_val['target'].values\n",
    "    train_dataset = custom_dataset(x_train, cfg)\n",
    "    valid_dataset = custom_dataset(x_val, cfg)\n",
    "    \n",
    "    print(\"build dataloader\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = True, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    # Add special token\n",
    "    # print(f'vocab_size={model.model.config.vocab_size}, tokenizer_len={len(cfg.tokenizer)}')\n",
    "    # model.model.resize_token_embeddings(len(cfg.tokenizer))\n",
    "    # print(f'Add special token, vocab_size={model.model.config.vocab_size}, tokenizer_len={len(cfg.tokenizer)}')\n",
    "    model.to(device)\n",
    "    # Optimizer\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model, \n",
    "        encoder_lr = cfg.encoder_lr, \n",
    "        decoder_lr = cfg.decoder_lr,\n",
    "        weight_decay = cfg.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr = cfg.encoder_lr, \n",
    "        eps = cfg.eps, \n",
    "        betas = cfg.betas\n",
    "    )\n",
    "    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n",
    "    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "    # Scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps = num_warmup_steps, \n",
    "        num_training_steps = num_train_steps, \n",
    "        num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    # Training & Validation loop\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "    best_score = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f'Epoch {epoch+1} start train')\n",
    "        start_time = time.time()\n",
    "        # Train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "        # Compute f2_score\n",
    "        print(f'Epoch {epoch+1} start get best threshold')\n",
    "        score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'predictions': predictions}, \n",
    "                f\"{cfg.model.replace('/', '-')}_fold{fold}_rerank_exp21_top100_5models.pth\"\n",
    "                )\n",
    "            val_predictions = predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Get best threshold\n",
    "    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "    LOGGER.info(f'Our CV score is {best_score} using a threshold of {best_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "444dea5b-f4bd-45af-8667-09ad54ea2162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "seed_everything(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "957e74c2-24f3-4cc0-86d5-4e3e80b89a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = combine_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1388a25f-7674-4794-bd1b-81ec343bb026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('autodl-tmp/recall_data_top100_fold0_exp21_full.csv', usecols=['topics_ids','content_ids','target','fold'])\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f37241-369c-4688-b4b9-8af48fc26596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change full_text to title\n",
    "# topics_df = pd.read_csv('topics.csv', usecols=['id', 'title']).fillna(\"\")\n",
    "# content_df = pd.read_csv('content.csv', usecols=['id', 'title']).fillna(\"\")\n",
    "# train = train.merge(topics_df, how='left', left_on='topics_ids', right_on='id').drop(['id'], axis=1).rename(columns={'title':'topic_title'})\n",
    "# train = train.merge(content_df, how='left', left_on='content_ids', right_on='id').drop(['id'], axis=1).rename(columns={'title':'content_title'})\n",
    "# train['text'] = train['topic_title'] + '[SEP]' + train['content_title']\n",
    "# train.drop(['topic_title', 'content_title'], axis=1, inplace=True)\n",
    "\n",
    "# del topics_df, content_df\n",
    "# gc.collect()\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b23062b9-9e97-4c8f-bc55-6d0da4351c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('autodl-tmp/recall_data_top100_fold0_exp21_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392dac11-c729-4406-a677-13e1f270d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('autodl-tmp/recall_data_top100_fold0_exp21_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a234b84c-e5cc-4187-8109-69f9d06b82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.read_csv('correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de04015-83a0-4bd7-8e09-132e5a67899c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "build dataloader\n",
      "Epoch 1 start train\n",
      "Epoch: [1][0/191842] Elapsed 0m 2s (remain 6562m 6s) Loss: 0.7001(0.7001) Grad: 5.7036  LR: 0.00000000  \n",
      "Epoch: [1][2000/191842] Elapsed 3m 48s (remain 362m 2s) Loss: 0.2815(0.5644) Grad: 3.1695  LR: 0.00000010  \n",
      "Epoch: [1][4000/191842] Elapsed 7m 37s (remain 357m 49s) Loss: 0.0582(0.3507) Grad: 0.6148  LR: 0.00000021  \n",
      "Epoch: [1][6000/191842] Elapsed 11m 24s (remain 353m 20s) Loss: 0.0089(0.2649) Grad: 0.2097  LR: 0.00000031  \n",
      "Epoch: [1][8000/191842] Elapsed 15m 13s (remain 349m 43s) Loss: 0.0097(0.2222) Grad: 0.2448  LR: 0.00000042  \n",
      "Epoch: [1][10000/191842] Elapsed 18m 59s (remain 345m 18s) Loss: 0.2324(0.1956) Grad: 1.2684  LR: 0.00000052  \n",
      "Epoch: [1][12000/191842] Elapsed 22m 47s (remain 341m 35s) Loss: 0.0105(0.1785) Grad: 0.2813  LR: 0.00000063  \n",
      "Epoch: [1][14000/191842] Elapsed 26m 33s (remain 337m 25s) Loss: 0.0697(0.1663) Grad: 0.6603  LR: 0.00000073  \n",
      "Epoch: [1][16000/191842] Elapsed 30m 20s (remain 333m 28s) Loss: 0.0887(0.1565) Grad: 0.8510  LR: 0.00000083  \n",
      "Epoch: [1][18000/191842] Elapsed 34m 8s (remain 329m 39s) Loss: 0.0923(0.1484) Grad: 1.0268  LR: 0.00000094  \n",
      "Epoch: [1][20000/191842] Elapsed 37m 55s (remain 325m 49s) Loss: 0.0061(0.1420) Grad: 0.2806  LR: 0.00000104  \n",
      "Epoch: [1][22000/191842] Elapsed 41m 41s (remain 321m 54s) Loss: 0.2079(0.1367) Grad: 1.6406  LR: 0.00000115  \n",
      "Epoch: [1][24000/191842] Elapsed 45m 27s (remain 317m 51s) Loss: 0.0778(0.1321) Grad: 0.7744  LR: 0.00000125  \n",
      "Epoch: [1][26000/191842] Elapsed 49m 13s (remain 313m 58s) Loss: 0.0841(0.1280) Grad: 1.3810  LR: 0.00000136  \n",
      "Epoch: [1][28000/191842] Elapsed 53m 1s (remain 310m 17s) Loss: 0.3098(0.1246) Grad: 2.5978  LR: 0.00000146  \n",
      "Epoch: [1][30000/191842] Elapsed 56m 47s (remain 306m 21s) Loss: 0.1900(0.1215) Grad: 3.0915  LR: 0.00000156  \n",
      "Epoch: [1][32000/191842] Elapsed 60m 32s (remain 302m 25s) Loss: 0.1327(0.1187) Grad: 2.0120  LR: 0.00000167  \n",
      "Epoch: [1][34000/191842] Elapsed 64m 16s (remain 298m 22s) Loss: 0.0397(0.1164) Grad: 1.8250  LR: 0.00000177  \n",
      "Epoch: [1][36000/191842] Elapsed 68m 4s (remain 294m 40s) Loss: 0.0048(0.1142) Grad: 0.2445  LR: 0.00000188  \n",
      "Epoch: [1][38000/191842] Elapsed 71m 52s (remain 290m 59s) Loss: 0.0039(0.1123) Grad: 0.1602  LR: 0.00000198  \n",
      "Epoch: [1][40000/191842] Elapsed 75m 38s (remain 287m 7s) Loss: 0.1429(0.1105) Grad: 2.7616  LR: 0.00000209  \n",
      "Epoch: [1][42000/191842] Elapsed 79m 25s (remain 283m 22s) Loss: 0.0068(0.1091) Grad: 0.2826  LR: 0.00000219  \n",
      "Epoch: [1][44000/191842] Elapsed 83m 10s (remain 279m 27s) Loss: 0.0036(0.1076) Grad: 0.1887  LR: 0.00000229  \n",
      "Epoch: [1][46000/191842] Elapsed 86m 55s (remain 275m 36s) Loss: 0.0950(0.1064) Grad: 1.7992  LR: 0.00000240  \n",
      "Epoch: [1][48000/191842] Elapsed 90m 41s (remain 271m 47s) Loss: 0.1287(0.1052) Grad: 2.4848  LR: 0.00000250  \n",
      "Epoch: [1][50000/191842] Elapsed 94m 28s (remain 267m 59s) Loss: 0.0842(0.1041) Grad: 1.7554  LR: 0.00000261  \n",
      "Epoch: [1][52000/191842] Elapsed 98m 12s (remain 264m 6s) Loss: 0.0044(0.1031) Grad: 0.1829  LR: 0.00000271  \n",
      "Epoch: [1][54000/191842] Elapsed 101m 58s (remain 260m 17s) Loss: 0.0889(0.1023) Grad: 3.2838  LR: 0.00000281  \n",
      "Epoch: [1][56000/191842] Elapsed 105m 44s (remain 256m 30s) Loss: 0.0938(0.1015) Grad: 1.4932  LR: 0.00000292  \n",
      "Epoch: [1][58000/191842] Elapsed 109m 30s (remain 252m 42s) Loss: 0.0408(0.1007) Grad: 2.5646  LR: 0.00000302  \n",
      "Epoch: [1][60000/191842] Elapsed 113m 19s (remain 249m 0s) Loss: 0.0023(0.1001) Grad: 0.0950  LR: 0.00000313  \n",
      "Epoch: [1][62000/191842] Elapsed 117m 5s (remain 245m 11s) Loss: 0.1696(0.0995) Grad: 2.2044  LR: 0.00000323  \n",
      "Epoch: [1][64000/191842] Elapsed 120m 51s (remain 241m 24s) Loss: 0.1938(0.0989) Grad: 1.8591  LR: 0.00000334  \n",
      "Epoch: [1][66000/191842] Elapsed 124m 37s (remain 237m 37s) Loss: 0.0986(0.0984) Grad: 0.8310  LR: 0.00000344  \n",
      "Epoch: [1][68000/191842] Elapsed 128m 26s (remain 233m 54s) Loss: 0.0005(0.0978) Grad: 0.0430  LR: 0.00000354  \n",
      "Epoch: [1][70000/191842] Elapsed 132m 10s (remain 230m 3s) Loss: 0.0019(0.0973) Grad: 0.0684  LR: 0.00000365  \n",
      "Epoch: [1][72000/191842] Elapsed 135m 54s (remain 226m 12s) Loss: 0.0791(0.0968) Grad: 1.3559  LR: 0.00000375  \n",
      "Epoch: [1][74000/191842] Elapsed 139m 39s (remain 222m 24s) Loss: 0.0862(0.0964) Grad: 2.4351  LR: 0.00000386  \n",
      "Epoch: [1][76000/191842] Elapsed 143m 27s (remain 218m 38s) Loss: 0.1024(0.0960) Grad: 1.1087  LR: 0.00000396  \n",
      "Epoch: [1][78000/191842] Elapsed 147m 11s (remain 214m 50s) Loss: 0.1216(0.0957) Grad: 2.7252  LR: 0.00000407  \n",
      "Epoch: [1][80000/191842] Elapsed 150m 56s (remain 211m 1s) Loss: 0.1145(0.0953) Grad: 1.5785  LR: 0.00000417  \n",
      "Epoch: [1][82000/191842] Elapsed 154m 43s (remain 207m 14s) Loss: 0.0542(0.0951) Grad: 3.0225  LR: 0.00000427  \n",
      "Epoch: [1][84000/191842] Elapsed 158m 28s (remain 203m 26s) Loss: 0.0067(0.0947) Grad: 1.5484  LR: 0.00000438  \n",
      "Epoch: [1][86000/191842] Elapsed 162m 12s (remain 199m 37s) Loss: 0.0100(0.0944) Grad: 0.5451  LR: 0.00000448  \n",
      "Epoch: [1][88000/191842] Elapsed 165m 59s (remain 195m 51s) Loss: 0.2424(0.0941) Grad: 1.6665  LR: 0.00000459  \n",
      "Epoch: [1][90000/191842] Elapsed 169m 44s (remain 192m 4s) Loss: 0.0025(0.0938) Grad: 0.0926  LR: 0.00000469  \n",
      "Epoch: [1][92000/191842] Elapsed 173m 29s (remain 188m 16s) Loss: 0.0062(0.0935) Grad: 0.6399  LR: 0.00000480  \n",
      "Epoch: [1][94000/191842] Elapsed 177m 16s (remain 184m 31s) Loss: 0.1014(0.0933) Grad: 1.1544  LR: 0.00000490  \n",
      "Epoch: [1][96000/191842] Elapsed 181m 2s (remain 180m 43s) Loss: 0.0455(0.0930) Grad: 2.6405  LR: 0.00000500  \n",
      "Epoch: [1][98000/191842] Elapsed 184m 46s (remain 176m 55s) Loss: 0.0244(0.0927) Grad: 0.9534  LR: 0.00000511  \n",
      "Epoch: [1][100000/191842] Elapsed 188m 29s (remain 173m 6s) Loss: 0.1120(0.0925) Grad: 1.7855  LR: 0.00000521  \n",
      "Epoch: [1][102000/191842] Elapsed 192m 16s (remain 169m 21s) Loss: 0.2302(0.0924) Grad: 1.2001  LR: 0.00000532  \n",
      "Epoch: [1][104000/191842] Elapsed 195m 59s (remain 165m 32s) Loss: 0.0004(0.0922) Grad: 0.0124  LR: 0.00000542  \n",
      "Epoch: [1][106000/191842] Elapsed 199m 44s (remain 161m 45s) Loss: 0.0035(0.0921) Grad: 0.1251  LR: 0.00000553  \n",
      "Epoch: [1][108000/191842] Elapsed 203m 31s (remain 157m 59s) Loss: 0.1312(0.0919) Grad: 3.2585  LR: 0.00000563  \n",
      "Epoch: [1][110000/191842] Elapsed 207m 16s (remain 154m 12s) Loss: 0.0140(0.0918) Grad: 0.9098  LR: 0.00000573  \n",
      "Epoch: [1][112000/191842] Elapsed 211m 1s (remain 150m 25s) Loss: 0.0152(0.0916) Grad: 1.9927  LR: 0.00000584  \n",
      "Epoch: [1][114000/191842] Elapsed 214m 45s (remain 146m 38s) Loss: 0.0009(0.0916) Grad: 0.0845  LR: 0.00000594  \n",
      "Epoch: [1][116000/191842] Elapsed 218m 30s (remain 142m 51s) Loss: 0.2337(0.0915) Grad: 2.5300  LR: 0.00000605  \n",
      "Epoch: [1][118000/191842] Elapsed 222m 17s (remain 139m 6s) Loss: 0.0058(0.0914) Grad: 0.5743  LR: 0.00000615  \n",
      "Epoch: [1][120000/191842] Elapsed 226m 2s (remain 135m 19s) Loss: 0.0960(0.0913) Grad: 1.1037  LR: 0.00000626  \n",
      "Epoch: [1][122000/191842] Elapsed 229m 47s (remain 131m 33s) Loss: 0.0374(0.0913) Grad: 1.6374  LR: 0.00000636  \n",
      "Epoch: [1][124000/191842] Elapsed 233m 34s (remain 127m 47s) Loss: 0.1172(0.0912) Grad: 0.9414  LR: 0.00000646  \n",
      "Epoch: [1][126000/191842] Elapsed 237m 20s (remain 124m 1s) Loss: 0.0004(0.0911) Grad: 0.0130  LR: 0.00000657  \n",
      "Epoch: [1][128000/191842] Elapsed 241m 6s (remain 120m 15s) Loss: 0.0889(0.0910) Grad: 5.3042  LR: 0.00000667  \n",
      "Epoch: [1][130000/191842] Elapsed 244m 51s (remain 116m 28s) Loss: 0.0434(0.0909) Grad: 2.9435  LR: 0.00000678  \n",
      "Epoch: [1][132000/191842] Elapsed 248m 37s (remain 112m 42s) Loss: 0.0052(0.0909) Grad: 0.5715  LR: 0.00000688  \n",
      "Epoch: [1][134000/191842] Elapsed 252m 24s (remain 108m 56s) Loss: 0.2160(0.0908) Grad: 1.3127  LR: 0.00000698  \n",
      "Epoch: [1][136000/191842] Elapsed 256m 8s (remain 105m 10s) Loss: 0.0984(0.0907) Grad: 1.4297  LR: 0.00000709  \n",
      "Epoch: [1][138000/191842] Elapsed 259m 54s (remain 101m 24s) Loss: 0.0012(0.0907) Grad: 0.0347  LR: 0.00000719  \n",
      "Epoch: [1][140000/191842] Elapsed 263m 37s (remain 97m 37s) Loss: 0.0063(0.0905) Grad: 0.3392  LR: 0.00000730  \n",
      "Epoch: [1][142000/191842] Elapsed 267m 22s (remain 93m 50s) Loss: 0.2792(0.0905) Grad: 1.3167  LR: 0.00000740  \n",
      "Epoch: [1][144000/191842] Elapsed 271m 7s (remain 90m 4s) Loss: 0.0955(0.0905) Grad: 2.1407  LR: 0.00000751  \n",
      "Epoch: [1][146000/191842] Elapsed 274m 53s (remain 86m 18s) Loss: 0.1553(0.0905) Grad: 3.5197  LR: 0.00000761  \n",
      "Epoch: [1][148000/191842] Elapsed 278m 38s (remain 82m 32s) Loss: 0.1128(0.0904) Grad: 4.2421  LR: 0.00000771  \n",
      "Epoch: [1][150000/191842] Elapsed 282m 25s (remain 78m 46s) Loss: 0.0149(0.0903) Grad: 0.5699  LR: 0.00000782  \n",
      "Epoch: [1][152000/191842] Elapsed 286m 9s (remain 75m 0s) Loss: 0.3603(0.0902) Grad: 2.5651  LR: 0.00000792  \n",
      "Epoch: [1][154000/191842] Elapsed 289m 56s (remain 71m 14s) Loss: 0.0857(0.0902) Grad: 0.8810  LR: 0.00000803  \n",
      "Epoch: [1][156000/191842] Elapsed 293m 41s (remain 67m 28s) Loss: 0.0960(0.0901) Grad: 0.9180  LR: 0.00000813  \n",
      "Epoch: [1][158000/191842] Elapsed 297m 27s (remain 63m 42s) Loss: 0.0065(0.0901) Grad: 0.8277  LR: 0.00000824  \n",
      "Epoch: [1][160000/191842] Elapsed 301m 13s (remain 59m 56s) Loss: 0.1660(0.0901) Grad: 1.3691  LR: 0.00000834  \n",
      "Epoch: [1][162000/191842] Elapsed 304m 57s (remain 56m 10s) Loss: 0.0868(0.0900) Grad: 1.9317  LR: 0.00000844  \n",
      "Epoch: [1][164000/191842] Elapsed 308m 44s (remain 52m 24s) Loss: 0.2359(0.0900) Grad: 3.8893  LR: 0.00000855  \n",
      "Epoch: [1][166000/191842] Elapsed 312m 29s (remain 48m 38s) Loss: 0.1586(0.0900) Grad: 1.7974  LR: 0.00000865  \n",
      "Epoch: [1][168000/191842] Elapsed 316m 13s (remain 44m 52s) Loss: 0.2456(0.0899) Grad: 2.7736  LR: 0.00000876  \n",
      "Epoch: [1][170000/191842] Elapsed 320m 0s (remain 41m 6s) Loss: 0.0407(0.0898) Grad: 3.2771  LR: 0.00000886  \n",
      "Epoch: [1][172000/191842] Elapsed 323m 45s (remain 37m 20s) Loss: 0.0039(0.0897) Grad: 0.2856  LR: 0.00000897  \n",
      "Epoch: [1][174000/191842] Elapsed 327m 32s (remain 33m 34s) Loss: 0.2087(0.0897) Grad: 1.5002  LR: 0.00000907  \n",
      "Epoch: [1][176000/191842] Elapsed 331m 15s (remain 29m 48s) Loss: 0.0002(0.0897) Grad: 0.0053  LR: 0.00000917  \n",
      "Epoch: [1][178000/191842] Elapsed 335m 2s (remain 26m 3s) Loss: 0.0757(0.0897) Grad: 3.6186  LR: 0.00000928  \n",
      "Epoch: [1][180000/191842] Elapsed 338m 47s (remain 22m 17s) Loss: 0.0577(0.0896) Grad: 2.6249  LR: 0.00000938  \n",
      "Epoch: [1][182000/191842] Elapsed 342m 35s (remain 18m 31s) Loss: 0.1332(0.0895) Grad: inf  LR: 0.00000949  \n",
      "Epoch: [1][184000/191842] Elapsed 346m 20s (remain 14m 45s) Loss: 0.0005(0.0895) Grad: 0.0289  LR: 0.00000959  \n",
      "Epoch: [1][186000/191842] Elapsed 350m 6s (remain 10m 59s) Loss: 0.0539(0.0895) Grad: 2.8896  LR: 0.00000970  \n",
      "Epoch: [1][188000/191842] Elapsed 353m 52s (remain 7m 13s) Loss: 0.0188(0.0894) Grad: 2.4670  LR: 0.00000980  \n",
      "Epoch: [1][190000/191842] Elapsed 357m 37s (remain 3m 27s) Loss: 0.0329(0.0894) Grad: 5.3725  LR: 0.00000990  \n",
      "Epoch: [1][191841/191842] Elapsed 361m 6s (remain 0m 0s) Loss: 0.1724(0.0894) Grad: 1.7534  LR: 0.00001000  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bf47d12c71448e8fd7cf113f6d100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/48332] Elapsed 0m 1s (remain 1429m 9s) Loss: 0.6380(0.6380) \n",
      "EVAL: [2000/48332] Elapsed 0m 38s (remain 15m 0s) Loss: 0.2341(0.2192) \n",
      "EVAL: [4000/48332] Elapsed 1m 22s (remain 15m 16s) Loss: 0.0656(0.1869) \n",
      "EVAL: [6000/48332] Elapsed 2m 5s (remain 14m 46s) Loss: 0.1222(0.1710) \n",
      "EVAL: [8000/48332] Elapsed 2m 45s (remain 13m 52s) Loss: 0.0005(0.1591) \n",
      "EVAL: [10000/48332] Elapsed 3m 24s (remain 13m 4s) Loss: 0.1169(0.1542) \n",
      "EVAL: [12000/48332] Elapsed 4m 9s (remain 12m 35s) Loss: 0.2145(0.1533) \n",
      "EVAL: [14000/48332] Elapsed 5m 1s (remain 12m 20s) Loss: 0.1781(0.1543) \n",
      "EVAL: [16000/48332] Elapsed 5m 52s (remain 11m 53s) Loss: 0.0032(0.1561) \n",
      "EVAL: [18000/48332] Elapsed 6m 38s (remain 11m 12s) Loss: 0.3083(0.1595) \n",
      "EVAL: [20000/48332] Elapsed 7m 14s (remain 10m 16s) Loss: 0.0006(0.1598) \n",
      "EVAL: [22000/48332] Elapsed 8m 5s (remain 9m 40s) Loss: 0.0008(0.1481) \n",
      "EVAL: [24000/48332] Elapsed 8m 51s (remain 8m 58s) Loss: 0.0015(0.1384) \n",
      "EVAL: [26000/48332] Elapsed 9m 39s (remain 8m 17s) Loss: 0.0002(0.1297) \n",
      "EVAL: [28000/48332] Elapsed 10m 38s (remain 7m 43s) Loss: 0.0007(0.1218) \n",
      "EVAL: [30000/48332] Elapsed 11m 24s (remain 6m 58s) Loss: 0.0019(0.1167) \n",
      "EVAL: [32000/48332] Elapsed 12m 15s (remain 6m 15s) Loss: 0.0005(0.1105) \n",
      "EVAL: [34000/48332] Elapsed 13m 9s (remain 5m 32s) Loss: 0.0002(0.1046) \n",
      "EVAL: [36000/48332] Elapsed 14m 14s (remain 4m 52s) Loss: 0.0006(0.0994) \n",
      "EVAL: [38000/48332] Elapsed 15m 6s (remain 4m 6s) Loss: 0.0020(0.0948) \n",
      "EVAL: [40000/48332] Elapsed 15m 57s (remain 3m 19s) Loss: 0.0005(0.0906) \n",
      "EVAL: [42000/48332] Elapsed 16m 58s (remain 2m 33s) Loss: 0.0009(0.0867) \n",
      "EVAL: [44000/48332] Elapsed 17m 54s (remain 1m 45s) Loss: 0.0003(0.0834) \n",
      "EVAL: [46000/48332] Elapsed 18m 53s (remain 0m 57s) Loss: 0.0002(0.0804) \n",
      "EVAL: [48000/48332] Elapsed 20m 2s (remain 0m 8s) Loss: 0.0004(0.0774) \n",
      "EVAL: [48331/48332] Elapsed 20m 9s (remain 0m 0s) Loss: 0.0016(0.0770) \n",
      "Epoch 1 start get best threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0894  avg_val_loss: 0.0770  time: 23683s\n",
      "Epoch 1 - Score: 0.3951 - Threshold: 0.00700\n",
      "Epoch 1 - Save Best Score: 0.3951 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 start train\n",
      "Epoch: [2][0/191842] Elapsed 0m 1s (remain 6115m 46s) Loss: 0.0046(0.0046) Grad: 0.7005  LR: 0.00001000  \n",
      "Epoch: [2][2000/191842] Elapsed 3m 44s (remain 355m 7s) Loss: 0.2849(0.0853) Grad: 2.6948  LR: 0.00001000  \n",
      "Epoch: [2][4000/191842] Elapsed 7m 32s (remain 353m 44s) Loss: 0.0003(0.0858) Grad: 0.0167  LR: 0.00001000  \n",
      "Epoch: [2][6000/191842] Elapsed 11m 17s (remain 349m 41s) Loss: 0.0004(0.0859) Grad: 0.0114  LR: 0.00001000  \n",
      "Epoch: [2][8000/191842] Elapsed 15m 2s (remain 345m 48s) Loss: 0.0512(0.0857) Grad: 3.0009  LR: 0.00001000  \n",
      "Epoch: [2][10000/191842] Elapsed 18m 49s (remain 342m 12s) Loss: 0.0682(0.0865) Grad: 2.0958  LR: 0.00001000  \n",
      "Epoch: [2][12000/191842] Elapsed 22m 34s (remain 338m 22s) Loss: 0.1046(0.0867) Grad: 1.3413  LR: 0.00001000  \n",
      "Epoch: [2][14000/191842] Elapsed 26m 18s (remain 334m 14s) Loss: 0.0920(0.0864) Grad: 2.0612  LR: 0.00001000  \n",
      "Epoch: [2][16000/191842] Elapsed 30m 5s (remain 330m 41s) Loss: 0.1249(0.0862) Grad: 0.8915  LR: 0.00001000  \n",
      "Epoch: [2][18000/191842] Elapsed 33m 52s (remain 327m 11s) Loss: 0.0050(0.0862) Grad: 0.6199  LR: 0.00001000  \n",
      "Epoch: [2][20000/191842] Elapsed 37m 36s (remain 323m 10s) Loss: 0.0018(0.0861) Grad: 0.0507  LR: 0.00001000  \n",
      "Epoch: [2][22000/191842] Elapsed 41m 23s (remain 319m 34s) Loss: 0.0002(0.0860) Grad: 0.0070  LR: 0.00001000  \n",
      "Epoch: [2][24000/191842] Elapsed 45m 10s (remain 315m 51s) Loss: 0.0012(0.0862) Grad: 0.0395  LR: 0.00001000  \n",
      "Epoch: [2][26000/191842] Elapsed 48m 56s (remain 312m 7s) Loss: 0.1784(0.0860) Grad: 1.1350  LR: 0.00000999  \n",
      "Epoch: [2][28000/191842] Elapsed 52m 43s (remain 308m 30s) Loss: 0.1438(0.0860) Grad: 1.4507  LR: 0.00000999  \n",
      "Epoch: [2][30000/191842] Elapsed 56m 30s (remain 304m 52s) Loss: 0.2336(0.0860) Grad: 1.9027  LR: 0.00000999  \n",
      "Epoch: [2][32000/191842] Elapsed 60m 19s (remain 301m 21s) Loss: 0.0219(0.0859) Grad: 0.8429  LR: 0.00000999  \n",
      "Epoch: [2][34000/191842] Elapsed 64m 5s (remain 297m 30s) Loss: 0.1132(0.0858) Grad: 0.6611  LR: 0.00000999  \n",
      "Epoch: [2][36000/191842] Elapsed 67m 52s (remain 293m 48s) Loss: 0.2260(0.0858) Grad: 3.2604  LR: 0.00000999  \n",
      "Epoch: [2][38000/191842] Elapsed 71m 40s (remain 290m 9s) Loss: 0.2011(0.0858) Grad: 1.8061  LR: 0.00000999  \n",
      "Epoch: [2][40000/191842] Elapsed 75m 26s (remain 286m 23s) Loss: 0.1522(0.0857) Grad: 8.2165  LR: 0.00000999  \n",
      "Epoch: [2][42000/191842] Elapsed 79m 11s (remain 282m 32s) Loss: 0.2072(0.0857) Grad: 1.9384  LR: 0.00000999  \n",
      "Epoch: [2][44000/191842] Elapsed 82m 56s (remain 278m 40s) Loss: 0.2016(0.0857) Grad: 2.4755  LR: 0.00000998  \n",
      "Epoch: [2][46000/191842] Elapsed 86m 39s (remain 274m 44s) Loss: 0.1282(0.0856) Grad: 0.9245  LR: 0.00000998  \n",
      "Epoch: [2][48000/191842] Elapsed 90m 23s (remain 270m 52s) Loss: 0.0005(0.0857) Grad: 0.0215  LR: 0.00000998  \n",
      "Epoch: [2][50000/191842] Elapsed 94m 11s (remain 267m 12s) Loss: 0.0001(0.0857) Grad: 0.0026  LR: 0.00000998  \n",
      "Epoch: [2][52000/191842] Elapsed 97m 57s (remain 263m 25s) Loss: 0.1330(0.0856) Grad: 0.9277  LR: 0.00000998  \n",
      "Epoch: [2][54000/191842] Elapsed 101m 40s (remain 259m 32s) Loss: 0.2018(0.0856) Grad: 3.0438  LR: 0.00000998  \n",
      "Epoch: [2][56000/191842] Elapsed 105m 26s (remain 255m 46s) Loss: 0.0009(0.0856) Grad: 0.0328  LR: 0.00000997  \n",
      "Epoch: [2][58000/191842] Elapsed 109m 10s (remain 251m 55s) Loss: 0.0896(0.0857) Grad: 1.1195  LR: 0.00000997  \n",
      "Epoch: [2][60000/191842] Elapsed 112m 55s (remain 248m 8s) Loss: 0.2011(0.0856) Grad: 3.0224  LR: 0.00000997  \n",
      "Epoch: [2][62000/191842] Elapsed 116m 43s (remain 244m 26s) Loss: 0.3683(0.0856) Grad: 3.8954  LR: 0.00000997  \n",
      "Epoch: [2][64000/191842] Elapsed 120m 27s (remain 240m 36s) Loss: 0.0003(0.0856) Grad: 0.0101  LR: 0.00000997  \n",
      "Epoch: [2][66000/191842] Elapsed 124m 15s (remain 236m 54s) Loss: 0.2849(0.0856) Grad: 2.8485  LR: 0.00000996  \n",
      "Epoch: [2][68000/191842] Elapsed 128m 0s (remain 233m 8s) Loss: 0.0004(0.0856) Grad: 0.0096  LR: 0.00000996  \n",
      "Epoch: [2][70000/191842] Elapsed 131m 47s (remain 229m 24s) Loss: 0.0005(0.0856) Grad: 0.0177  LR: 0.00000996  \n",
      "Epoch: [2][72000/191842] Elapsed 135m 36s (remain 225m 42s) Loss: 0.1288(0.0856) Grad: 5.6857  LR: 0.00000996  \n",
      "Epoch: [2][74000/191842] Elapsed 139m 23s (remain 221m 57s) Loss: 0.0881(0.0856) Grad: 1.0933  LR: 0.00000995  \n",
      "Epoch: [2][76000/191842] Elapsed 143m 9s (remain 218m 11s) Loss: 0.1048(0.0855) Grad: 1.1969  LR: 0.00000995  \n",
      "Epoch: [2][78000/191842] Elapsed 146m 54s (remain 214m 24s) Loss: 0.1078(0.0856) Grad: 1.3232  LR: 0.00000995  \n",
      "Epoch: [2][80000/191842] Elapsed 150m 41s (remain 210m 40s) Loss: 0.0237(0.0856) Grad: 3.1089  LR: 0.00000995  \n",
      "Epoch: [2][82000/191842] Elapsed 154m 28s (remain 206m 55s) Loss: 0.0006(0.0856) Grad: 0.0268  LR: 0.00000994  \n",
      "Epoch: [2][84000/191842] Elapsed 158m 15s (remain 203m 10s) Loss: 0.1138(0.0856) Grad: 1.2278  LR: 0.00000994  \n",
      "Epoch: [2][86000/191842] Elapsed 162m 0s (remain 199m 23s) Loss: 0.1102(0.0856) Grad: 2.3635  LR: 0.00000994  \n",
      "Epoch: [2][88000/191842] Elapsed 165m 47s (remain 195m 37s) Loss: 0.0002(0.0856) Grad: 0.0101  LR: 0.00000994  \n",
      "Epoch: [2][90000/191842] Elapsed 169m 33s (remain 191m 51s) Loss: 0.0006(0.0855) Grad: 0.0597  LR: 0.00000993  \n",
      "Epoch: [2][92000/191842] Elapsed 173m 19s (remain 188m 5s) Loss: 0.0002(0.0855) Grad: 0.0092  LR: 0.00000993  \n",
      "Epoch: [2][94000/191842] Elapsed 177m 5s (remain 184m 19s) Loss: 0.0010(0.0855) Grad: 0.1212  LR: 0.00000993  \n",
      "Epoch: [2][96000/191842] Elapsed 180m 50s (remain 180m 32s) Loss: 0.0993(0.0855) Grad: 2.1320  LR: 0.00000992  \n",
      "Epoch: [2][98000/191842] Elapsed 184m 35s (remain 176m 45s) Loss: 0.1374(0.0855) Grad: 1.7344  LR: 0.00000992  \n",
      "Epoch: [2][100000/191842] Elapsed 188m 21s (remain 172m 59s) Loss: 0.0915(0.0855) Grad: 1.7276  LR: 0.00000992  \n",
      "Epoch: [2][102000/191842] Elapsed 192m 9s (remain 169m 15s) Loss: 0.0652(0.0855) Grad: 5.7439  LR: 0.00000991  \n",
      "Epoch: [2][104000/191842] Elapsed 195m 57s (remain 165m 30s) Loss: 0.2062(0.0854) Grad: 1.9278  LR: 0.00000991  \n",
      "Epoch: [2][106000/191842] Elapsed 199m 44s (remain 161m 45s) Loss: 0.1163(0.0854) Grad: 1.7728  LR: 0.00000991  \n",
      "Epoch: [2][108000/191842] Elapsed 203m 31s (remain 157m 59s) Loss: 0.0011(0.0854) Grad: 0.0547  LR: 0.00000990  \n",
      "Epoch: [2][110000/191842] Elapsed 207m 18s (remain 154m 14s) Loss: 0.2251(0.0854) Grad: 1.8371  LR: 0.00000990  \n",
      "Epoch: [2][112000/191842] Elapsed 211m 7s (remain 150m 30s) Loss: 0.1072(0.0854) Grad: 0.4660  LR: 0.00000990  \n",
      "Epoch: [2][114000/191842] Elapsed 214m 55s (remain 146m 44s) Loss: 0.0011(0.0854) Grad: 0.1532  LR: 0.00000989  \n",
      "Epoch: [2][116000/191842] Elapsed 218m 41s (remain 142m 58s) Loss: 0.1536(0.0854) Grad: 4.7507  LR: 0.00000989  \n",
      "Epoch: [2][118000/191842] Elapsed 222m 28s (remain 139m 12s) Loss: 0.0039(0.0853) Grad: 0.7560  LR: 0.00000989  \n",
      "Epoch: [2][120000/191842] Elapsed 226m 14s (remain 135m 26s) Loss: 0.0888(0.0853) Grad: 1.4934  LR: 0.00000988  \n",
      "Epoch: [2][122000/191842] Elapsed 230m 0s (remain 131m 40s) Loss: 0.1024(0.0853) Grad: 1.5369  LR: 0.00000988  \n",
      "Epoch: [2][124000/191842] Elapsed 233m 48s (remain 127m 54s) Loss: 0.0003(0.0853) Grad: 0.0071  LR: 0.00000987  \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate one fold\n",
    "train_and_evaluate_one_fold(train, correlations, 0, CFG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
