{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is where I train a SBERT model (more specifically, a **SimCSE** model for **unsupervised** learning).\n\n**Steps:**\n* Reconstruct the `correlations.csv` to make it more straight-forward\n* Explore the tree, then use `title` from `root_node` to the current `topic_node` as `input_1`\n* Use `title` of each `content_item` as `input_2`\n* Use the imformation above to make my dataset\n* Train a SBERT model and add it as a dataset for future use","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom IPython.display import display, Markdown\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:01:09.611039Z","iopub.execute_input":"2023-01-09T13:01:09.611493Z","iopub.status.idle":"2023-01-09T13:01:10.156057Z","shell.execute_reply.started":"2023-01-09T13:01:09.611402Z","shell.execute_reply":"2023-01-09T13:01:10.155120Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Bulid my Dataset","metadata":{}},{"cell_type":"markdown","source":"## Reconstruct Correlations\n\nFirst, considering the old `correlations.csv` file is not a ideal form for training (each row contains one `topic_id` and several `content_ids`), let's reconstruct it.","metadata":{}},{"cell_type":"code","source":"correlations_df = pd.read_csv(DATA_PATH + 'correlations.csv')\ncorrelations_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:22.287482Z","iopub.execute_input":"2023-01-09T11:28:22.288377Z","iopub.status.idle":"2023-01-09T11:28:22.445858Z","shell.execute_reply.started":"2023-01-09T11:28:22.288337Z","shell.execute_reply":"2023-01-09T11:28:22.444968Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         topic_id                                        content_ids\n0  t_00004da3a1b2  c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...\n1  t_00068291e9a4  c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...\n2  t_00069b63a70a                                     c_11a1dc0bfb99\n3  t_0006d41a73a8  c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...\n4  t_0008768bdee6       c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00069b63a70a</td>\n      <td>c_11a1dc0bfb99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_0008768bdee6</td>\n      <td>c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"correlation = correlations_df.copy()\ncorrelation.content_ids = correlation.content_ids.str.split()\ncorrelation = correlation.explode('content_ids').rename(columns={\"content_ids\": \"content_id\"}).reset_index(drop=True)\ncorrelation.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:25.200971Z","iopub.execute_input":"2023-01-09T11:28:25.201340Z","iopub.status.idle":"2023-01-09T11:28:25.370977Z","shell.execute_reply.started":"2023-01-09T11:28:25.201309Z","shell.execute_reply":"2023-01-09T11:28:25.369861Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         topic_id      content_id\n0  t_00004da3a1b2  c_1108dd0c7a5d\n1  t_00004da3a1b2  c_376c5a8eb028\n2  t_00004da3a1b2  c_5bc0e1e2cba0\n3  t_00004da3a1b2  c_76231f9d0b5e\n4  t_00068291e9a4  c_639ea2ef9c95\n5  t_00068291e9a4  c_89ce9367be10\n6  t_00068291e9a4  c_ac1672cdcd2c\n7  t_00068291e9a4  c_ebb7fdf10a7e\n8  t_00069b63a70a  c_11a1dc0bfb99\n9  t_0006d41a73a8  c_0c6473c3480d","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_376c5a8eb028</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_76231f9d0b5e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>t_00068291e9a4</td>\n      <td>c_89ce9367be10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>t_00068291e9a4</td>\n      <td>c_ac1672cdcd2c</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>t_00068291e9a4</td>\n      <td>c_ebb7fdf10a7e</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>t_00069b63a70a</td>\n      <td>c_11a1dc0bfb99</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_0c6473c3480d</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now it has become a dataframe `correlation` with 279919 rows and 2 columns. Each row contrains only one `topic_id` and one `content_id`.","metadata":{}},{"cell_type":"markdown","source":"## Explore the Topic Tree\n\nNext, we consider using the semantic information in each `topic_tree`. \n\nWe can use some helpful code from the notebook **Tips and Recommendations from Hosts**.","metadata":{}},{"cell_type":"code","source":"topics_df = pd.read_csv(DATA_PATH + 'topics.csv', index_col=0).fillna({\"title\": \"\"})  # use `id` as index\ntopics_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:28.321845Z","iopub.execute_input":"2023-01-09T11:28:28.322544Z","iopub.status.idle":"2023-01-09T11:28:28.792043Z","shell.execute_reply.started":"2023-01-09T11:28:28.322502Z","shell.execute_reply":"2023-01-09T11:28:28.789977Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                            title  \\\nid                                                                  \nt_00004da3a1b2                         Откриването на резисторите   \nt_000095e03056             Unit 3.3 Enlargements and Similarities   \nt_00068291e9a4                    Entradas e saídas de uma função   \nt_00069b63a70a                                        Transcripts   \nt_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                                      description channel  \\\nid                                                                          \nt_00004da3a1b2  Изследване на материали, които предизвикват на...  000cf7   \nt_000095e03056                                                NaN  b3f329   \nt_00068291e9a4               Entenda um pouco mais sobre funções.  8e286a   \nt_00069b63a70a                                                NaN  6e3ba4   \nt_0006d41a73a8  Научи повече за графиките на сложните показате...  000cf7   \n\n               category  level language          parent  has_content  \nid                                                                    \nt_00004da3a1b2   source      4       bg  t_16e29365b50d         True  \nt_000095e03056  aligned      2       en  t_aa32fb6252dc        False  \nt_00068291e9a4   source      4       pt  t_d14b6c2a2b70         True  \nt_00069b63a70a   source      3       en  t_4054df11a74e         True  \nt_0006d41a73a8   source      4       bg  t_e2452e21d252         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>description</th>\n      <th>channel</th>\n      <th>category</th>\n      <th>level</th>\n      <th>language</th>\n      <th>parent</th>\n      <th>has_content</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>t_00004da3a1b2</th>\n      <td>Откриването на резисторите</td>\n      <td>Изследване на материали, които предизвикват на...</td>\n      <td>000cf7</td>\n      <td>source</td>\n      <td>4</td>\n      <td>bg</td>\n      <td>t_16e29365b50d</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>t_000095e03056</th>\n      <td>Unit 3.3 Enlargements and Similarities</td>\n      <td>NaN</td>\n      <td>b3f329</td>\n      <td>aligned</td>\n      <td>2</td>\n      <td>en</td>\n      <td>t_aa32fb6252dc</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>t_00068291e9a4</th>\n      <td>Entradas e saídas de uma função</td>\n      <td>Entenda um pouco mais sobre funções.</td>\n      <td>8e286a</td>\n      <td>source</td>\n      <td>4</td>\n      <td>pt</td>\n      <td>t_d14b6c2a2b70</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>t_00069b63a70a</th>\n      <td>Transcripts</td>\n      <td>NaN</td>\n      <td>6e3ba4</td>\n      <td>source</td>\n      <td>3</td>\n      <td>en</td>\n      <td>t_4054df11a74e</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>t_0006d41a73a8</th>\n      <td>Графики на експоненциални функции (Алгебра 2 н...</td>\n      <td>Научи повече за графиките на сложните показате...</td>\n      <td>000cf7</td>\n      <td>source</td>\n      <td>4</td>\n      <td>bg</td>\n      <td>t_e2452e21d252</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# define some helper functions and classes to aid with data traversal\ndef print_markdown(md):\n    display(Markdown(md))\n\nclass Topic:\n    def __init__(self, topic_id):\n        self.id = topic_id\n\n    @property\n    def parent(self):\n        parent_id = topics_df.loc[self.id].parent\n        if pd.isna(parent_id):\n            return None\n        else:\n            return Topic(parent_id)\n\n    @property\n    def ancestors(self):\n        ancestors = []\n        parent = self.parent\n        while parent is not None:\n            ancestors.append(parent)\n            parent = parent.parent\n        return ancestors\n\n    def get_breadcrumbs(self, separator=\"[SEP]\", include_self=True, include_root=True):\n        ancestors = self.ancestors\n        if include_self:\n            ancestors = [self] + ancestors\n        if not include_root:\n            ancestors = ancestors[:-1]\n        return separator.join(reversed([a.title for a in ancestors]))\n\n    def __eq__(self, other):\n        if not isinstance(other, Topic):\n            return False\n        return self.id == other.id\n\n    def __getattr__(self, name):\n        return topics_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<Topic(id={self.id}, title=\\\"{self.title}\\\")>\"","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-01-09T11:28:32.469288Z","iopub.execute_input":"2023-01-09T11:28:32.469790Z","iopub.status.idle":"2023-01-09T11:28:32.486015Z","shell.execute_reply.started":"2023-01-09T11:28:32.469755Z","shell.execute_reply":"2023-01-09T11:28:32.484929Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# An example\ntopic = Topic(\"t_00004da3a1b2\")\nprint(\"Topic title:\\t'\" + topic.title + \"'\")\nprint(\"Breadcrumbs:\\t\" + topic.get_breadcrumbs())","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:33.676881Z","iopub.execute_input":"2023-01-09T11:28:33.677575Z","iopub.status.idle":"2023-01-09T11:28:33.742493Z","shell.execute_reply.started":"2023-01-09T11:28:33.677516Z","shell.execute_reply":"2023-01-09T11:28:33.739817Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Topic title:\t'Откриването на резисторите'\nBreadcrumbs:\tKhan Academy (български език)[SEP]Наука[SEP]Физика[SEP]Открития и проекти[SEP]Откриването на резисторите\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can use this topic_tree to get more semantic information.\n\nFor example, for the given topic `Откриването на резисторите`, we can use the hole title string from the root node to the current topic node `Khan Academy (български език) >> Наука >> Физика >> Открития и проекти >> Откриването на резисторите` to calculate the similarity between this topic and other content items.\n\nIs this a better way than using just the single title information from one topic?","metadata":{}},{"cell_type":"markdown","source":"## Build Dataset\n\nBefore training our SBERT model, we need to prepare our data in certain formats.\n\nSince it's an unsupervised task (with no label indicating how similar two sentences are), we can use a SimCSE model (another sentence-transformers-model for unsupervised learning).\n\nSentenceTransformers implements the `MultipleNegativesRankingLoss`, which makes training with SimCSE trivial.\n\n>**Links for more information:**\n>\n>[SimCSE](https://www.sbert.net/examples/unsupervised_learning/SimCSE/README.html)\n>\n>[How to train sentence-transformers](https://huggingface.co/blog/how-to-train-sentence-transformers)","metadata":{}},{"cell_type":"markdown","source":"We can explore an example dataset to find out the required format.","metadata":{}},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-01-09T08:20:29.210283Z","iopub.execute_input":"2023-01-09T08:20:29.210645Z","iopub.status.idle":"2023-01-09T08:20:40.070486Z","shell.execute_reply.started":"2023-01-09T08:20:29.210616Z","shell.execute_reply":"2023-01-09T08:20:40.069199Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (8.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.11.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (22.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.13.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.13)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"embedding-data/sentence-compression\")","metadata":{"execution":{"iopub.status.busy":"2023-01-09T08:21:17.425668Z","iopub.execute_input":"2023-01-09T08:21:17.426025Z","iopub.status.idle":"2023-01-09T08:21:22.383218Z","shell.execute_reply.started":"2023-01-09T08:21:17.425997Z","shell.execute_reply":"2023-01-09T08:21:22.382087Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/embedding-data--sentence-compression to /root/.cache/huggingface/datasets/json/embedding-data--sentence-compression-305c6d7a2ac8f95b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6e11b7b4a04118b324872513113530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08b4bd104764877959faba4368f6765"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f8d41a472942dd8d212919a50bb515"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/embedding-data--sentence-compression-305c6d7a2ac8f95b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c9af25e05184150a3687db9221c4113"}},"metadata":{}}]},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T08:27:01.411231Z","iopub.execute_input":"2023-01-09T08:27:01.411616Z","iopub.status.idle":"2023-01-09T08:27:01.417064Z","shell.execute_reply.started":"2023-01-09T08:27:01.411585Z","shell.execute_reply":"2023-01-09T08:27:01.415950Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['set'],\n        num_rows: 180000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"- The dataset has {dataset['train'].num_rows} examples.\")\nprint(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\nprint(f\"- Examples look like this: {dataset['train'][0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-09T08:22:46.622243Z","iopub.execute_input":"2023-01-09T08:22:46.623057Z","iopub.status.idle":"2023-01-09T08:22:46.629821Z","shell.execute_reply.started":"2023-01-09T08:22:46.623012Z","shell.execute_reply":"2023-01-09T08:22:46.628865Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"- The dataset has 180000 examples.\n- Each example is a <class 'dict'> with a <class 'list'> as value.\n- Examples look like this: {'set': [\"The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League's two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints.\", 'USHL completes expansion draft']}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After having a good knowledge about the required format, we can now make our own training dataset.","metadata":{}},{"cell_type":"code","source":"correlation_new = correlation.copy()\ncorrelation_new.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:47.503309Z","iopub.execute_input":"2023-01-09T11:28:47.503681Z","iopub.status.idle":"2023-01-09T11:28:47.517116Z","shell.execute_reply.started":"2023-01-09T11:28:47.503647Z","shell.execute_reply":"2023-01-09T11:28:47.516019Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         topic_id      content_id\n0  t_00004da3a1b2  c_1108dd0c7a5d\n1  t_00004da3a1b2  c_376c5a8eb028\n2  t_00004da3a1b2  c_5bc0e1e2cba0\n3  t_00004da3a1b2  c_76231f9d0b5e\n4  t_00068291e9a4  c_639ea2ef9c95","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_376c5a8eb028</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_76231f9d0b5e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can also construct a ContentItem class to get its title.","metadata":{}},{"cell_type":"code","source":"content_df = pd.read_csv(DATA_PATH + 'content.csv', index_col=0).fillna({\"title\": \"\"})  # use `id` as index","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:28:51.982036Z","iopub.execute_input":"2023-01-09T11:28:51.982406Z","iopub.status.idle":"2023-01-09T11:29:10.171479Z","shell.execute_reply.started":"2023-01-09T11:28:51.982375Z","shell.execute_reply":"2023-01-09T11:29:10.170489Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ContentItem:\n    def __init__(self, content_id):\n        self.id = content_id\n        \n    def __getattr__(self, name):\n        return content_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<ContentItem(id={self.id}, title=\\\"{self.title}\\\")>\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ContentItem):\n            return False\n        return self.id == other.id\n    \n    def get_title(self):\n        return self.title","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:29:10.173339Z","iopub.execute_input":"2023-01-09T11:29:10.173803Z","iopub.status.idle":"2023-01-09T11:29:10.181667Z","shell.execute_reply.started":"2023-01-09T11:29:10.173765Z","shell.execute_reply":"2023-01-09T11:29:10.180516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Put them together to make our `input_new` dataframe!\n* `topic_id`: the id of one topic\n* `content_id`: the id of one content\n* `topic_input`: titles from the root node to the topic node with the above `topic_id`\n* `content_input`: title of the content with the above `content_id`","metadata":{}},{"cell_type":"code","source":"input_df = pd.DataFrame(columns=['topic_input', 'content_input'])\n\nfor index, row in tqdm(correlation_new.iterrows(), total=correlation_new.shape[0]):\n    top = Topic(row['topic_id'])\n    con = ContentItem(row['content_id'])\n    input_df.loc[index] = (top.get_breadcrumbs(), con.get_title())\n    \ninput_new = pd.concat([correlation_new, input_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T11:29:10.183248Z","iopub.execute_input":"2023-01-09T11:29:10.183877Z","iopub.status.idle":"2023-01-09T12:28:39.905765Z","shell.execute_reply.started":"2023-01-09T11:29:10.183832Z","shell.execute_reply":"2023-01-09T12:28:39.904820Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/279919 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d729d910445e4526b09a57be40e806c3"}},"metadata":{}}]},{"cell_type":"code","source":"# check whether we need to drop certain rows\nfor index, row in tqdm(input_new.iterrows(), total=input_new.shape[0]):\n    if row['topic_input'] == \"\" or row['content_input'] == \"\":\n        print('yes')\n        break","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:28:39.908306Z","iopub.execute_input":"2023-01-09T12:28:39.908691Z","iopub.status.idle":"2023-01-09T12:28:54.613227Z","shell.execute_reply.started":"2023-01-09T12:28:39.908653Z","shell.execute_reply":"2023-01-09T12:28:54.612197Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/279919 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d3d08af8da43a18d148af55fded7ea"}},"metadata":{}}]},{"cell_type":"markdown","source":"We can save the `input_new` dataframe as a csv file. If we want to train other models, we can just load it as our dataset. This will largely decrease total computing time.","metadata":{}},{"cell_type":"code","source":"input_new.to_csv('input_new_sbert_baseline.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:29:11.135406Z","iopub.execute_input":"2023-01-09T12:29:11.135802Z","iopub.status.idle":"2023-01-09T12:29:12.377334Z","shell.execute_reply.started":"2023-01-09T12:29:11.135767Z","shell.execute_reply":"2023-01-09T12:29:12.376358Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Of course every time you want to use this data file, you can just load it.","metadata":{}},{"cell_type":"code","source":"input_new = pd.read_csv('/kaggle/input/input-new-sbert-baseline/input_new_sbert_baseline.csv').reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:07.924545Z","iopub.execute_input":"2023-01-09T13:02:07.924910Z","iopub.status.idle":"2023-01-09T13:02:08.795155Z","shell.execute_reply.started":"2023-01-09T13:02:07.924878Z","shell.execute_reply":"2023-01-09T13:02:08.794022Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"input_new.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:08.969424Z","iopub.execute_input":"2023-01-09T13:02:08.971700Z","iopub.status.idle":"2023-01-09T13:02:08.982364Z","shell.execute_reply.started":"2023-01-09T13:02:08.971666Z","shell.execute_reply":"2023-01-09T13:02:08.981297Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0        topic_id      content_id  \\\n0           0  t_00004da3a1b2  c_1108dd0c7a5d   \n1           1  t_00004da3a1b2  c_376c5a8eb028   \n2           2  t_00004da3a1b2  c_5bc0e1e2cba0   \n3           3  t_00004da3a1b2  c_76231f9d0b5e   \n4           4  t_00068291e9a4  c_639ea2ef9c95   \n\n                                         topic_input  \\\n0  Khan Academy (български език)[SEP]Наука[SEP]Фи...   \n1  Khan Academy (български език)[SEP]Наука[SEP]Фи...   \n2  Khan Academy (български език)[SEP]Наука[SEP]Фи...   \n3  Khan Academy (български език)[SEP]Наука[SEP]Фи...   \n4  Khan Academy (Português (Brasil))[SEP]Matemáti...   \n\n                                       content_input  \n0                                Молив като резистор  \n1                 Да чуем променливото съпротивление  \n2     Променлив резистор (реостат) с графит от молив  \n3  Последователно свързване на галваничен елемент...  \n4            Dados e resultados de funções: gráficos  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>topic_id</th>\n      <th>content_id</th>\n      <th>topic_input</th>\n      <th>content_input</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d</td>\n      <td>Khan Academy (български език)[SEP]Наука[SEP]Фи...</td>\n      <td>Молив като резистор</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>t_00004da3a1b2</td>\n      <td>c_376c5a8eb028</td>\n      <td>Khan Academy (български език)[SEP]Наука[SEP]Фи...</td>\n      <td>Да чуем променливото съпротивление</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>t_00004da3a1b2</td>\n      <td>c_5bc0e1e2cba0</td>\n      <td>Khan Academy (български език)[SEP]Наука[SEP]Фи...</td>\n      <td>Променлив резистор (реостат) с графит от молив</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>t_00004da3a1b2</td>\n      <td>c_76231f9d0b5e</td>\n      <td>Khan Academy (български език)[SEP]Наука[SEP]Фи...</td>\n      <td>Последователно свързване на галваничен елемент...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95</td>\n      <td>Khan Academy (Português (Brasil))[SEP]Matemáti...</td>\n      <td>Dados e resultados de funções: gráficos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# sentence lists for input\ntopic_input_list = input_new.iloc[:, 2].to_list()\ncontent_input_list = input_new.iloc[:, 3].to_list()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:24.516286Z","iopub.execute_input":"2023-01-09T13:02:24.516864Z","iopub.status.idle":"2023-01-09T13:02:24.529486Z","shell.execute_reply.started":"2023-01-09T13:02:24.516828Z","shell.execute_reply":"2023-01-09T13:02:24.528470Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Then use these two lists to make Dataset.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:27.940010Z","iopub.execute_input":"2023-01-09T13:02:27.940508Z","iopub.status.idle":"2023-01-09T13:02:27.945550Z","shell.execute_reply.started":"2023-01-09T13:02:27.940453Z","shell.execute_reply":"2023-01-09T13:02:27.944274Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/sentence-transformers-offline-install/sentence-transformers')\nimport sentence_transformers\nfrom sentence_transformers import InputExample, losses, SentenceTransformer\nfrom torch.utils.data import DataLoader\n\n# Convert train sentences to sentence pairs\ntrain_data = [InputExample(texts=[t, c]) for t, c in zip(topic_input_list, content_input_list)]\n\n# DataLoader to batch your data\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:31.935362Z","iopub.execute_input":"2023-01-09T13:02:31.936152Z","iopub.status.idle":"2023-01-09T13:02:37.299864Z","shell.execute_reply.started":"2023-01-09T13:02:31.936116Z","shell.execute_reply":"2023-01-09T13:02:37.298739Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Train the SBERT Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:42.655105Z","iopub.execute_input":"2023-01-09T13:02:42.655826Z","iopub.status.idle":"2023-01-09T13:02:42.660338Z","shell.execute_reply.started":"2023-01-09T13:02:42.655784Z","shell.execute_reply":"2023-01-09T13:02:42.659081Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# config\nMODEL_PATH = \"/kaggle/input/sbert-models/paraphrase-multilingual-mpnet-base-v2\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS = 5\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:51.483959Z","iopub.execute_input":"2023-01-09T13:02:51.484349Z","iopub.status.idle":"2023-01-09T13:02:51.553827Z","shell.execute_reply.started":"2023-01-09T13:02:51.484316Z","shell.execute_reply":"2023-01-09T13:02:51.551882Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# model\nmodel = SentenceTransformer(MODEL_PATH)\nmodel.to(device)\n\n# loss\ntrain_loss = losses.MultipleNegativesRankingLoss(model)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:02:57.824911Z","iopub.execute_input":"2023-01-09T13:02:57.825659Z","iopub.status.idle":"2023-01-09T13:03:14.106545Z","shell.execute_reply.started":"2023-01-09T13:02:57.825620Z","shell.execute_reply":"2023-01-09T13:03:14.105559Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# clear memory\n!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=EPOCHS, \n    show_progress_bar=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:04:01.100768Z","iopub.execute_input":"2023-01-09T13:04:01.101140Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941e0bf4c1054f3a96e47b6b74b6afd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/17495 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09fbbf4519484682b2f00cfd5a335fc0"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now we've trained our SBERT (SimCSE) model! Let's save it for future use!","metadata":{}},{"cell_type":"code","source":"# save your model\nmodel.save('/kaggle/working/sbert-baseline-model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What to do next?\n* Use Swifter to accelerate the processing of Dataframe","metadata":{}}]}